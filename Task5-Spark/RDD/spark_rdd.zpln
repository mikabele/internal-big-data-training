{
  "paragraphs": [
    {
      "text": "\nhdfs dfs -mkdir /tmp/data\n\nhdfs dfs -rm -r -skipTrash  /tmp/fund\nhdfs dfs -rm -r -skipTrash  /tmp/stg_fund\nhdfs dfs -rm -r -skipTrash  /tmp/audit\nhdfs dfs -rm -r -skipTrash  /tmp/landing_fund\nhdfs dfs -rm -r -skipTrash /tmp/tmp_file\nhdfs dfs -rm -r -skipTrash /tmp/test\n\nhdfs dfs -mkdir /tmp/fund\nhdfs dfs -mkdir /tmp/stg_fund\nhdfs dfs -mkdir /tmp/audit\nhdfs dfs -mkdir /tmp/landing_fund\n\nhdfs dfs -mkdir /tmp/audit/landing_audit_table\nhdfs dfs -mkdir /tmp/audit/stg_audit_table\nhdfs dfs -mkdir /tmp/audit/fund_audit_table\nhdfs dfs -mkdir /tmp/fund/monthly_table\nhdfs dfs -mkdir /tmp/stg_fund/stg_fund_table\nhdfs dfs -mkdir /tmp/landing_fund/landing_fund_table\n\nhdfs dfs -touchz /tmp/audit/landing_audit_table/part\nhdfs dfs -touchz /tmp/audit/stg_audit_table/part\nhdfs dfs -touchz /tmp/audit/fund_audit_table/part\nhdfs dfs -touchz /tmp/fund/monthly_table/part\nhdfs dfs -touchz /tmp/stg_fund/stg_fund_table/part\nhdfs dfs -touchz /tmp/landing_fund/landing_fund_table/part",
      "user": "anonymous",
      "dateUpdated": "2021-07-05T19:46:22+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "mkdir: `/tmp/data': File exists\nDeleted /tmp/fund\nDeleted /tmp/stg_fund\nDeleted /tmp/audit\nDeleted /tmp/landing_fund\nrm: `/tmp/tmp_file': No such file or directory\nrm: `/tmp/test': No such file or directory\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625131556905_1119955132",
      "id": "paragraph_1624962613455_1728988261",
      "dateCreated": "2021-07-01T09:25:56+0000",
      "dateStarted": "2021-07-04T22:52:24+0000",
      "dateFinished": "2021-07-04T22:53:19+0000",
      "status": "FINISHED",
      "focus": true,
      "$$hashKey": "object:115777"
    },
    {
      "text": "%spark\n\nimport java.time.LocalDateTime\nimport java.time.format.DateTimeFormatter\nimport java.io.File\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{FileSystem, Path}\nimport java.security.{MessageDigest,DigestInputStream}\nimport java.nio.file.{Files, Paths}\nimport java.text.SimpleDateFormat\nimport java.util.Date\nimport java.util.Calendar\nimport java.util.Arrays",
      "user": "anonymous",
      "dateUpdated": "2021-07-05T14:49:39+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import java.time.LocalDateTime\nimport java.time.format.DateTimeFormatter\nimport java.io.File\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{FileSystem, Path}\nimport java.security.{MessageDigest, DigestInputStream}\nimport java.nio.file.{Files, Paths}\nimport java.text.SimpleDateFormat\nimport java.util.Date\nimport java.util.Calendar\nimport java.util.Arrays\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625131556906_2085474909",
      "id": "paragraph_1624963031581_1468154265",
      "dateCreated": "2021-07-01T09:25:56+0000",
      "dateStarted": "2021-07-05T14:49:39+0000",
      "dateFinished": "2021-07-05T14:49:40+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115778"
    },
    {
      "text": "%spark\n\n//var startTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"YYYY-MM-dd HH:mm:ss\"))\nval full=false\nval task1 = \"load data\"\nvar openClose = false",
      "user": "anonymous",
      "dateUpdated": "2021-07-04T20:02:42+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mfull\u001b[0m: \u001b[1m\u001b[32mBoolean\u001b[0m = false\n\u001b[1m\u001b[34mtask1\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = load data\n\u001b[1m\u001b[34mopenClose\u001b[0m: \u001b[1m\u001b[32mBoolean\u001b[0m = false\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625131556907_350785281",
      "id": "paragraph_1624963435876_1865625660",
      "dateCreated": "2021-07-01T09:25:56+0000",
      "dateStarted": "2021-07-04T20:02:42+0000",
      "dateFinished": "2021-07-04T20:02:42+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115779"
    },
    {
      "text": "%spark\n\ndef checkSumSha256(path: Path): String = {\n    val hdfs = FileSystem.get(new Configuration())\n    \n    val buffer = new Array[Byte](8192)\n    val sha256 = MessageDigest.getInstance(\"SHA-256\")\n    \n    val dis = new DigestInputStream(hdfs.open(path),sha256)\n    try{\n        while(dis.read(buffer) != -1)\n        {}\n    }\n    finally{\n        dis.close()\n    }\n    \n    sha256.digest.map(\"%02x\".format(_)).mkString\n}",
      "user": "anonymous",
      "dateUpdated": "2021-07-04T20:03:22+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mcheckSumSha256\u001b[0m: \u001b[1m\u001b[32m(path: org.apache.hadoop.fs.Path)String\u001b[0m\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625131556907_518954109",
      "id": "paragraph_1625040849213_1344792736",
      "dateCreated": "2021-07-01T09:25:56+0000",
      "dateStarted": "2021-07-04T20:03:22+0000",
      "dateFinished": "2021-07-04T20:03:22+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115780"
    },
    {
      "text": "%spark\n\nval test = sc.parallelize(Array((\"12,2\",\"13,0\")))\ntest.saveAsTextFile(\"/tmp/test5\")\ntest.collect()\nval readTest = sc.textFile(\"/tmp/test5/part*\")\nreadTest.collect()\n//arr(0)(0)",
      "user": "anonymous",
      "dateUpdated": "2021-07-04T21:59:16+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mtest\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(String, String)]\u001b[0m = ParallelCollectionRDD[932] at parallelize at <console>:62\n\u001b[1m\u001b[34mreadTest\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[String]\u001b[0m = /tmp/test5/part* MapPartitionsRDD[935] at textFile at <console>:65\n\u001b[1m\u001b[34mres82\u001b[0m: \u001b[1m\u001b[32mArray[String]\u001b[0m = Array((12,2,13,0))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=219",
              "$$hashKey": "object:118150"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=220",
              "$$hashKey": "object:118151"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=221",
              "$$hashKey": "object:118152"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625430859076_1038575113",
      "id": "paragraph_1625430859076_1038575113",
      "dateCreated": "2021-07-04T20:34:19+0000",
      "dateStarted": "2021-07-04T21:59:16+0000",
      "dateFinished": "2021-07-04T21:59:16+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115781"
    },
    {
      "text": "%spark\n\nval market=\"XAUUSD\"\nvar startTimestamp : String = _\nvar endTimestamp : String = _\nvar count : Long = _\nval fs = FileSystem.get(new Configuration())\nval files = fs.listStatus(new Path(\"/tmp/data\"))\nvar rddLandingFund : org.apache.spark.rdd.RDD[Array[String]] = _\n\nif (!full) {\n    rddLandingFund = sc.textFile(\"/tmp/landing_fund/landing_fund_table/part*\").map(line => line.split(\";\"))\n}\n\nvar rddLandingAudit = sc.textFile(\"/tmp/audit/landing_audit_table/part*\").map(line => line.split(\";\"))\n\nfor (file<-files){\n    \n    startTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\n    val checkSum = checkSumSha256(file.getPath)\n    val checkLog = rddLandingAudit.filter(line => line(2) != checkSum).count()\n\n    if (full || checkLog == 0) {\n        val rddNewStartRecord = sc.parallelize(Array(Array(file.getPath.toString,checkSum,startTimestamp,market,task1,\"landing_fund_db\",null,null)))\n        rddLandingAudit = rddLandingAudit.union(rddNewStartRecord)\n        \n        rddLandingAudit.map(line => line.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\n        fs.delete(new Path(\"/tmp/audit/landing_audit_table\"),true)\n        fs.rename(new Path(\"/tmp/tmp_file\"),new Path(\"/tmp/audit/landing_audit_table\"))\n        rddLandingAudit = sc.textFile(\"/tmp/audit/landing_audit_table/part*\").map(line => line.split(\";\"))\n        \n        \n        if (rddLandingFund == null) {\n            rddLandingFund = sc.textFile(file.getPath.toString).map(line => line.split(\";\")).map(line =>  line++Array(startTimestamp,market))\n            count = rddLandingFund.count()\n        }\n        else {\n            var rddTemp = sc.textFile(file.getPath.toString).map(line => line.split(\";\")).map(line =>  line++Array(startTimestamp,market))\n            val yearMarketPairs = rddTemp.map(line => (line(0),line(6))).distinct().collect()\n            rddLandingFund = rddLandingFund.filter(line => !(yearMarketPairs contains (line(0),line(6))))\n            rddLandingFund = rddLandingFund.union(rddTemp)\n            count = rddTemp.count()\n        }\n        \n        \n        rddLandingFund.map(line => line.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\n        fs.delete(new Path(\"/tmp/landing_fund/landing_fund_table\"),true)\n        fs.rename(new Path(\"/tmp/tmp_file\"),new Path(\"/tmp/landing_fund/landing_fund_table\"))\n        rddLandingFund = sc.textFile(\"/tmp/landing_fund/landing_fund_table/part*\").map(line => line.split(\";\"))\n        \n        \n        endTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"YYYY-MM-dd HH:mm:ss\"))\n        val rddNewEndRecord = sc.parallelize(Array(Array(file.getPath.toString,checkSum,startTimestamp,market,task1,\"landing_fund_db\",endTimestamp,count.toString)))\n        rddLandingAudit = rddLandingAudit.filter(line => line(2) != startTimestamp)\n        rddLandingAudit = rddLandingAudit.union(rddNewEndRecord)\n        \n        rddLandingAudit.map(line => line.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\n        fs.delete(new Path(\"/tmp/audit/landing_audit_table\"),true)\n        fs.rename(new Path(\"/tmp/tmp_file\"),new Path(\"/tmp/audit/landing_audit_table\"))\n        rddLandingAudit = sc.textFile(\"/tmp/audit/landing_audit_table/part*\").map(line => line.split(\";\"))\n    }\n    \n}",
      "user": "anonymous",
      "dateUpdated": "2021-07-04T22:53:41+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mmarket\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = XAUUSD\n\u001b[1m\u001b[34mstartTimestamp\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2021-07-04 22:53:42\n\u001b[1m\u001b[34mendTimestamp\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2021-07-04 22:53:42\n\u001b[1m\u001b[34mcount\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 1446\n\u001b[1m\u001b[34mfs\u001b[0m: \u001b[1m\u001b[32morg.apache.hadoop.fs.FileSystem\u001b[0m = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-937280027_18, ugi=zeppelin (auth:SIMPLE)]]\n\u001b[1m\u001b[34mfiles\u001b[0m: \u001b[1m\u001b[32mArray[org.apache.hadoop.fs.FileStatus]\u001b[0m = Array(FileStatus{path=hdfs://cluster-cd71-m/tmp/data/test_dataset.csv; isDirectory=false; length=80877; replication=1; blocksize=134217728; modification_time=1625131791917; access_time=1625438702583; owner=mikhail_belevich; group=hadoop; permission=rw-r--r--; isSymlink=false})\n\u001b[1m\u001b[34mrddLandingFund\u001b[0m: \u001b[1m\u001b[32morg.apache.spark...\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=267",
              "$$hashKey": "object:118218"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=268",
              "$$hashKey": "object:118219"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=269",
              "$$hashKey": "object:118220"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=270",
              "$$hashKey": "object:118221"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=271",
              "$$hashKey": "object:118222"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=272",
              "$$hashKey": "object:118223"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625131556907_52483398",
      "id": "paragraph_1624961337006_2014420463",
      "dateCreated": "2021-07-01T09:25:56+0000",
      "dateStarted": "2021-07-04T22:53:41+0000",
      "dateFinished": "2021-07-04T22:53:43+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115782"
    },
    {
      "text": "%spark\n\nrddLandingAudit = sc.textFile(\"/tmp/audit/landing_audit_table/part*\").map(line => line.split(\";\"))\nrddLandingAudit.take(10)",
      "user": "anonymous",
      "dateUpdated": "2021-07-04T21:37:36+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "rddLandingAudit: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[884] at map at <console>:61\n\u001b[1m\u001b[34mres75\u001b[0m: \u001b[1m\u001b[32mArray[Array[String]]\u001b[0m = Array(Array(hdfs://cluster-cd71-m/tmp/data/test_dataset.csv, 589ce53a5034727b0bebfcc62e850f29d4da65eab4832da5834137b6275f33cb, 2021-07-04 21:32:35, XAUUSD, load data, landing_fund_db, 2021-07-04 21:32:36, 1446))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=205",
              "$$hashKey": "object:118301"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=206",
              "$$hashKey": "object:118302"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=207",
              "$$hashKey": "object:118303"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625131556907_1630105098",
      "id": "paragraph_1625045634835_616431492",
      "dateCreated": "2021-07-01T09:25:56+0000",
      "dateStarted": "2021-07-04T21:37:36+0000",
      "dateFinished": "2021-07-04T21:37:36+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115783"
    },
    {
      "text": "%spark\n\nrddLandingFund.take(3)",
      "user": "anonymous",
      "dateUpdated": "2021-07-04T21:32:53+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres72\u001b[0m: \u001b[1m\u001b[32mArray[Array[String]]\u001b[0m = Array(Array(Time (UTC), Open, High, Low, Close, \"Volume \", 2021-07-04 21:32:35, XAUUSD), Array(2012-01-16 00:00:00, 1291,4, 1291,5, 1291,4, 1291,4, 0, 2021-07-04 21:32:35, XAUUSD), Array(2012-01-16 00:00:10, 1291,4, 1291,4, 1291,4, 1291,4, 0, 2021-07-04 21:32:35, XAUUSD))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=199",
              "$$hashKey": "object:118369"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=200",
              "$$hashKey": "object:118370"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625131556908_487641400",
      "id": "paragraph_1624962543015_1679332715",
      "dateCreated": "2021-07-01T09:25:56+0000",
      "dateStarted": "2021-07-04T21:32:53+0000",
      "dateFinished": "2021-07-04T21:32:53+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115784"
    },
    {
      "text": "%spark\n\nvar rddStgAudit = sc.textFile(\"/tmp/audit/stg_audit_table/part*\").map(line => line.split(\";\"))\n\nval recordCount = rddStgAudit.count()\n//var lastRecordStgAudit :Array[String] = _\nvar lastStartDate : Date = _\nif (recordCount != 0){\n    val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n    lastStartDate = format.parse(rddStgAudit.collect().last(2))\n}\n\nval startTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\nval rddNewStartRecord = sc.parallelize(Array(Array(startTimestamp,null,\"load data from landing to staging\",0.toString)))\nrddStgAudit = rddStgAudit.union(rddNewStartRecord)\n\nrddStgAudit.map(line => line.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\nfs.delete(new Path(\"/tmp/audit/stg_audit_table\"),true)\nfs.rename(new Path(\"/tmp/tmp_file\"),new Path(\"/tmp/audit/stg_audit_table\"))\nrddStgAudit = sc.textFile(\"/tmp/audit/stg_audit_table/part*\").map(line => line.split(\";\"))\n\n//var rddClearFund : org.apache.spark.rdd.RDD[(Date, Float, Float, String, String)] = _\nvar rddNewLandingFund = rddLandingFund.map(line => {\n    val market = line(7)\n    val loadDate = line(6)\n    val time = line(0)\n    val open = line(1)\n    val close = line(2)\n    ((market,loadDate),(time,open,close))\n})\n\nval rddPrepareToJoinAudit = rddLandingAudit.map(line => {\n    val startTime = line(2)\n    val market =line(3)\n    val endTime = line(6)\n    ((market,startTime),endTime)\n})\n\nvar rddJoinedTimestamp = rddNewLandingFund.join(rddPrepareToJoinAudit).filter(line => {\n    val endTime = line._2._2\n    endTime != null\n})\n\nif (!full && recordCount != 0) {\n    rddJoinedTimestamp = rddJoinedTimestamp.filter(line => {\n        val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n        val startTime = format.parse(line._1._1)\n        startTime.after(lastStartDate)\n    }) \n}\n\n\nvar rddClearFund = rddJoinedTimestamp.map( line => {\n        try {\n            val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n            val time = format.parse(line._2._1._1)\n            //val time = line(0)\n            val open : Float= line._2._1._2.replace(\",\",\".\").toFloat\n            val close : Float = line._2._1._3.replace(\",\",\".\").toFloat\n            val market = line._1._1\n            val startTimestamp = format.parse(line._1._2)\n            (time,open,close,market,startTimestamp)\n        }\n        catch {\n            case e => {\n                (null,0f,0f,null,null)\n            }\n        }\n    })\n    \nval count = rddClearFund.count()    \n    \nvar rddStgFund = sc.textFile(\"/tmp/stg_fund/stg_fund_table/part*\").map(line => {\n    try {\n            val values = line.split(\";\")\n            val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n            val time = format.parse(values(0))\n            val open : Float = values(1).toFloat\n            val close : Float = values(2).toFloat\n            val market = values(3)\n            val startTimestamp = format.parse(values(4))\n            (time,open,close,market,startTimestamp)\n        }\n        catch {\n            case e => {\n                (null,0f,0f,null,null)\n            }\n        }\n})\n\nval marketYearPairs = rddClearFund.map(line => (line._3,line._4)).distinct().collect()\nrddStgFund = rddStgFund.filter(line => !(marketYearPairs contains (line._3,line._4)))\nrddStgFund = rddStgFund.union(rddClearFund).filter(line => line._1 != null && line._2 != 0 && line._3 != 0 && line._4 != null && line._5 != null)\n\nrddStgFund.map(line => {\n    val dateFormat = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n    val date = dateFormat.format(line._1)\n    val startTimestamp = dateFormat.format(line._5)\n    (date,line._2,line._3,line._4,startTimestamp).productIterator.mkString(\";\")\n}).saveAsTextFile(\"/tmp/tmp_file\")\nfs.delete(new Path(\"/tmp/stg_fund/stg_fund_table\"),true)\nfs.rename(new Path(\"/tmp/tmp_file\"),new Path(\"/tmp/stg_fund/stg_fund_table\"))\nrddStgFund = sc.textFile(\"/tmp/stg_fund/stg_fund_table/part*\").map(line => {\n    try {\n            val values = line.split(\";\")\n            val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n            val time = format.parse(values(0))\n            val open : Float = values(1).toFloat\n            val close : Float = values(2).toFloat\n            val market = values(3)\n            val startTimestamp = format.parse(values(4))\n            (time,open,close,market,startTimestamp)\n        }\n        catch {\n            case e => {\n                (null,0f,0f,null,null)\n            }\n        }\n})\n\nval endTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"YYYY-MM-dd HH:mm:ss\"))\nval rddNewEndRecord = sc.parallelize(Array(Array(startTimestamp,endTimestamp,\"load data from landing to staging\",count.toString)))\nrddStgAudit = rddStgAudit.filter(line => line(0) != startTimestamp)\nrddStgAudit = rddStgAudit.union(rddNewEndRecord)\n\nrddStgAudit.map(line => line.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\nfs.delete(new Path(\"/tmp/audit/stg_audit_table\"),true)\nfs.rename(new Path(\"/tmp/tmp_file\"),new Path(\"/tmp/audit/stg_audit_table\"))\nrddStgAudit = sc.textFile(\"/tmp/audit/stg_audit_table/part*\").map(line => line.split(\";\"))",
      "user": "anonymous",
      "dateUpdated": "2021-07-04T22:53:46+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "<console>:155: \u001b[33mwarning: \u001b[0mThis catches all Throwables. If this is really intended, use `case e : Throwable` to clear this warning.\n                   case e => {\n                        ^\n<console>:175: \u001b[33mwarning: \u001b[0mThis catches all Throwables. If this is really intended, use `case e : Throwable` to clear this warning.\n                   case e => {\n                        ^\n<console>:205: \u001b[33mwarning: \u001b[0mThis catches all Throwables. If this is really intended, use `case e : Throwable` to clear this warning.\n                   case e => {\n                        ^\n\u001b[1m\u001b[34mrddStgAudit\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[Array[String]]\u001b[0m = MapPartitionsRDD[1181] at map at <console>:219\n\u001b[1m\u001b[34mrecordCount\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 0\n\u001b[1m\u001b[34mlastStartDate\u001b[0m: \u001b[1m\u001b[32mjava.util.Date\u001b[0m = null\n\u001b[1m\u001b[34mstartTimestamp\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2021-07-04 22:53:47\n\u001b[1m\u001b[34mrddNewStartRecord\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[Array[String]]\u001b[0m = ParallelCollectionRDD[1145] at parallelize at <console>:104\nrddStgAudit: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[1181] at map at <console>:219\nrddStgAudit: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[1181] at map at <console>:219\n\u001b[1m\u001b[34mrddNewLandingFund\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[((String, String), (String, String, String))]\u001b[0m = MapPartitionsRDD[1...\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=273",
              "$$hashKey": "object:118432"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=274",
              "$$hashKey": "object:118433"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=275",
              "$$hashKey": "object:118434"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=276",
              "$$hashKey": "object:118435"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=277",
              "$$hashKey": "object:118436"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=278",
              "$$hashKey": "object:118437"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625131556908_2028597035",
      "id": "paragraph_1624963999888_2018019472",
      "dateCreated": "2021-07-01T09:25:56+0000",
      "dateStarted": "2021-07-04T22:53:46+0000",
      "dateFinished": "2021-07-04T22:53:48+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115785"
    },
    {
      "text": "%spark\n\nrddJoinedTimestamp.take(3)",
      "user": "anonymous",
      "dateUpdated": "2021-07-04T22:45:22+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres89\u001b[0m: \u001b[1m\u001b[32mArray[((String, String), ((String, String, String), String))]\u001b[0m = Array(((XAUUSD,2021-07-04 22:45:02),((Time (UTC),Open,High),2021-07-04 22:45:03)), ((XAUUSD,2021-07-04 22:45:02),((2012-01-16 00:00:00,1291,4,1291,5),2021-07-04 22:45:03)), ((XAUUSD,2021-07-04 22:45:02),((2012-01-16 00:00:10,1291,4,1291,4),2021-07-04 22:45:03)))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=245",
              "$$hashKey": "object:118515"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=246",
              "$$hashKey": "object:118516"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=247",
              "$$hashKey": "object:118517"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625131556908_1750607193",
      "id": "paragraph_1625089107000_1255509210",
      "dateCreated": "2021-07-01T09:25:56+0000",
      "dateStarted": "2021-07-04T22:45:23+0000",
      "dateFinished": "2021-07-04T22:45:23+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115786"
    },
    {
      "text": "%spark\n\nrddPrepareToJoinAudit.take(1)",
      "user": "anonymous",
      "dateUpdated": "2021-07-04T15:52:38+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres2\u001b[0m: \u001b[1m\u001b[32mArray[((String, String), String)]\u001b[0m = Array(((XAUUSD,2021-07-04 15:30:05),2021-07-04 15:30:08))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=6",
              "$$hashKey": "object:118583"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=7",
              "$$hashKey": "object:118584"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=8",
              "$$hashKey": "object:118585"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625131556908_1883949553",
      "id": "paragraph_1625088712112_683934722",
      "dateCreated": "2021-07-01T09:25:56+0000",
      "dateStarted": "2021-07-04T15:52:38+0000",
      "dateFinished": "2021-07-04T15:52:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115787"
    },
    {
      "text": "%spark\n\nrddClearFund.collect()",
      "user": "anonymous",
      "dateUpdated": "2021-07-04T22:45:27+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres90\u001b[0m: \u001b[1m\u001b[32mArray[(java.util.Date, Float, Float, String, java.util.Date)]\u001b[0m = Array((null,0.0,0.0,null,null), (Mon Jan 16 00:00:00 UTC 2012,1291.4,1291.5,XAUUSD,Sun Jul 04 22:45:02 UTC 2021), (Mon Jan 16 00:00:10 UTC 2012,1291.4,1291.4,XAUUSD,Sun Jul 04 22:45:02 UTC 2021), (Mon Jan 16 00:00:20 UTC 2012,1291.6,1291.6,XAUUSD,Sun Jul 04 22:45:02 UTC 2021), (Mon Jan 16 00:00:30 UTC 2012,1291.5,1291.5,XAUUSD,Sun Jul 04 22:45:02 UTC 2021), (Mon Jan 16 00:00:40 UTC 2012,1291.1,1291.4,XAUUSD,Sun Jul 04 22:45:02 UTC 2021), (Mon Jan 16 00:00:50 UTC 2012,1291.4,1291.4,XAUUSD,Sun Jul 04 22:45:02 UTC 2021), (Mon Jan 16 00:01:00 UTC 2012,1291.1,1291.1,XAUUSD,Sun Jul 04 22:45:02 UTC 2021), (Mon Jan 16 00:01:10 UTC 2012,1290.9,1290.9,XAUUSD,Sun Jul 04 22:45:02 UTC 2021), (Mon Jan 16 ...\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=248",
              "$$hashKey": "object:118651"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625131556908_207870673",
      "id": "paragraph_1625125252175_641405034",
      "dateCreated": "2021-07-01T09:25:56+0000",
      "dateStarted": "2021-07-04T22:45:27+0000",
      "dateFinished": "2021-07-04T22:45:28+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115788"
    },
    {
      "text": "%spark\n\nrddNewLandingFund.take(3)\n//rddPrepareToJoinAudit.take(1)",
      "user": "anonymous",
      "dateUpdated": "2021-07-04T15:19:30+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "<console>:35: \u001b[31merror: \u001b[0mnot found: value rddNewLandingFund\n       rddNewLandingFund.take(3)\n       ^\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625131556908_102177394",
      "id": "paragraph_1624964113277_234999328",
      "dateCreated": "2021-07-01T09:25:56+0000",
      "dateStarted": "2021-07-04T15:19:31+0000",
      "dateFinished": "2021-07-04T15:19:31+0000",
      "status": "ERROR",
      "$$hashKey": "object:115789"
    },
    {
      "text": "%spark\n\nrddStgAudit.collect()",
      "user": "anonymous",
      "dateUpdated": "2021-07-04T22:54:23+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres101\u001b[0m: \u001b[1m\u001b[32mArray[Array[String]]\u001b[0m = Array(Array(2021-07-04 22:53:47, 2021-07-04 22:53:48, load data from landing to staging, 1446))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=281",
              "$$hashKey": "object:118761"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625414007531_266734261",
      "id": "paragraph_1625414007531_266734261",
      "dateCreated": "2021-07-04T15:53:27+0000",
      "dateStarted": "2021-07-04T22:54:23+0000",
      "dateFinished": "2021-07-04T22:54:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115790"
    },
    {
      "text": "%spark\n\nrddStgFund.collect()",
      "user": "anonymous",
      "dateUpdated": "2021-07-04T22:53:58+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres100\u001b[0m: \u001b[1m\u001b[32mArray[(java.util.Date, Float, Float, String, java.util.Date)]\u001b[0m = Array((Mon Jan 16 00:00:00 UTC 2012,1291.4,1291.5,XAUUSD,Sun Jul 04 22:53:42 UTC 2021), (Mon Jan 16 00:00:10 UTC 2012,1291.4,1291.4,XAUUSD,Sun Jul 04 22:53:42 UTC 2021), (Mon Jan 16 00:00:20 UTC 2012,1291.6,1291.6,XAUUSD,Sun Jul 04 22:53:42 UTC 2021), (Mon Jan 16 00:00:30 UTC 2012,1291.5,1291.5,XAUUSD,Sun Jul 04 22:53:42 UTC 2021), (Mon Jan 16 00:00:40 UTC 2012,1291.1,1291.4,XAUUSD,Sun Jul 04 22:53:42 UTC 2021), (Mon Jan 16 00:00:50 UTC 2012,1291.4,1291.4,XAUUSD,Sun Jul 04 22:53:42 UTC 2021), (Mon Jan 16 00:01:00 UTC 2012,1291.1,1291.1,XAUUSD,Sun Jul 04 22:53:42 UTC 2021), (Mon Jan 16 00:01:10 UTC 2012,1290.9,1290.9,XAUUSD,Sun Jul 04 22:53:42 UTC 2021), (Mon Jan 16 00:01:20 UTC 2012,1291.1,...\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=280",
              "$$hashKey": "object:118819"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625414192520_1627669889",
      "id": "paragraph_1625414192520_1627669889",
      "dateCreated": "2021-07-04T15:56:32+0000",
      "dateStarted": "2021-07-04T22:53:58+0000",
      "dateFinished": "2021-07-04T22:53:58+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115791"
    },
    {
      "text": "%spark\n\nval openClose = false\nval market = \"XAUUSD\"\nvar rddFundAudit = sc.textFile(\"/tmp/audit/fund_audit_table/part*\").map(line => line.split(\";\"))\nval startTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\nval rddNewFundAuditRecord = sc.parallelize(Array(Array(startTimestamp,null,\"monthly overview\",market)))\nrddFundAudit = rddFundAudit.union(rddNewFundAuditRecord)\n\nrddFundAudit.map(line => line.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\nfs.delete(new Path(\"/tmp/audit/fund_audit_table\"),true)\nfs.rename(new Path(\"/tmp/tmp_file\"),new Path(\"/tmp/audit/fund_audit_table\"))\nrddFundAudit = sc.textFile(\"/tmp/audit/fund_audit_table/part*\").map(line => line.split(\";\"))\n\nvar rddMonthly : org.apache.spark.rdd.RDD[(String, String, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Boolean, String)] = _\n\nif (!full) {\n    rddMonthly = sc.textFile(\"/tmp/fund/monthly_table/part*\").map(line => {\n        val values = line.split(\";\")\n        (values(0),values(1),values(2).toFloat,values(3).toFloat,values(4).toFloat,values(5).toFloat,values(6).toFloat,values(7).toFloat,values(8).toFloat,values(9).toFloat,values(10).toFloat,values(11).toFloat,values(12).toFloat,values(13).toFloat,values(14).toFloat,values(15).toBoolean,values(16))\n    })\n    \n    val dates = rddMonthly.filter(line => {\n        val monthlyMarket = line._1\n        val monthlyOpenClose = line._16\n        monthlyMarket == market && monthlyOpenClose == openClose\n    }).map(line => {\n        val format = new SimpleDateFormat(\"YYYY-MM-dd HH:mm:ss\")\n        val time = format.parse(line._17)\n        time \n    }).collect()\n    var lastDate : Date = null \n    if (!dates.isEmpty) {\n        lastDate = dates.reduceLeft((x,y) => {\n            if (x.after(y)) {\n                x    \n            }\n            else {\n                y\n            }\n        })\n    }\n\n    val years = rddStgFund.filter(line => {\n        val stgMarket = line._4\n        val startTimestamp = line._5\n        stgMarket == market && (lastDate == null || startTimestamp.after(lastDate))\n    }).map(line => {\n        val cal = Calendar.getInstance\n        cal.setTime(line._1)\n        cal.get(Calendar.YEAR)\n    }).distinct().collect()\n\n    rddMonthly = rddMonthly.filter(line => !{\n        val monthlyMarket = line._1\n        val year = line._2\n        (year == \"total\" || (years contains year.toInt)) && monthlyMarket == market  \n    })\n    rddStgFund = rddStgFund.filter(line => {\n        val stgMarket = line._4\n        val cal = Calendar.getInstance\n        cal.setTime(line._1)\n        (years contains cal.get(Calendar.YEAR)) && stgMarket == market\n    })\n}\nelse {\n    rddStgFund = rddStgFund.filter(line => {\n        val stgMarket = line._4\n        stgMarket == market\n    })\n}\n\n\n\nval rddNewData = rddStgFund.map(line => {\n    val cal = Calendar.getInstance\n    cal.setTime(line._1)\n    ((cal.get(Calendar.YEAR),cal.get(Calendar.MONTH)),(line._1, line._2,line._3,line._4))\n})\n\n\nval rddMinDateInMonth = rddNewData.reduceByKey((x,y) => {\n    if (x._1.before(y._1)) {\n        x    \n    }\n    else {\n        y\n    }\n})\nval rddMaxDateInMonth = rddNewData.reduceByKey((x,y) => {\n    if (x._1.after(y._1)) {\n        x    \n    }\n    else {\n        y\n    }\n})\n\nval rddOpenClose = rddMinDateInMonth.join(rddMaxDateInMonth).sortByKey()\n\nvar prevClose : Float = -1\nval rddPrevClose = rddOpenClose.map(line => {\n    val temp = prevClose\n    prevClose = line._2._2._3\n    if (temp != -1) {\n        (line._1 , line._2 ,temp)\n    }\n    else {\n        (line._1 , line._2 ,line._2._1._2)\n    }\n})\n\nvar rddPercents : org.apache.spark.rdd.RDD[(Int,(Int,Float))] = _\nif (openClose) {\n    rddPercents = rddOpenClose.map(line => (line._1._1,(line._1._2,(line._2._2._3-line._2._1._2)/100)))\n}\nelse {\n    rddPercents = rddPrevClose.map(line => (line._1._1,(line._1._2,(line._2._2._3-line._3)/100)))\n}\n\nval rddPivotTable = rddPercents.groupByKey().mapValues(value => value.toList).map(line => {\n    var jan,feb,march,april,may,june,july,aug,sep,oct,nov,dec,total : Float = 0\n    for (value <- line._2) {\n        value._1 match {\n            case 0 => jan += value._2\n            case 1 => feb += value._2\n            case 2 => march += value._2\n            case 3 => april += value._2\n            case 4 => may += value._2\n            case 5 => june += value._2\n            case 6 => july += value._2\n            case 7 => aug += value._2\n            case 8 => sep += value._2\n            case 9 => oct += value._2\n            case 10 => nov += value._2\n            case 11 => dec += value._2\n        }\n        total+=value._2\n    }\n    (market,line._1.toString,jan,feb,march,april,may,june,july,aug,sep,oct,nov,dec,total,openClose,startTimestamp)\n})\n\n\nvar rddResult = rddPivotTable.union(rddMonthly)\n\nval count = rddResult.filter(line => {\n    val monthlyMarket = line._1\n    val monthlyOpenClose = line._16\n    monthlyMarket == market && monthlyOpenClose == openClose\n}).count()\nval rddTotal = sc.parallelize(Array(rddResult.filter(line => {\n    val monthlyMarket = line._1\n    val monthlyOpenClose = line._16\n    monthlyMarket == market && monthlyOpenClose == openClose\n}).reduce((x,y) => (market,\"total\",(x._3+y._3)/count,(x._4+y._4)/count,(x._5+y._5)/count,(x._6+y._6)/count,(x._7+y._7)/count,(x._8+y._8)/count,(x._9+y._9)/count,(x._10+y._10)/count,(x._11+y._11)/count,(x._12+y._12)/count,(x._13+y._13)/count,(x._14+y._14)/count,(x._15+y._15)/count,openClose,startTimestamp))))\n\nrddResult = rddResult.union(rddTotal)\n\nrddResult.map(line => line.productIterator.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\nfs.delete(new Path(\"/tmp/fund/monthly_table\"),true)\nfs.rename(new Path(\"/tmp/tmp_file\"),new Path(\"/tmp/fund/monthly_table\"))\nrddResult = sc.textFile(\"/tmp/fund/monthly_table/part*\").map(line => {\n    val values = line.split(\";\")\n    (values(0),values(1),values(2).toFloat,values(3).toFloat,values(4).toFloat,values(5).toFloat,values(6).toFloat,values(7).toFloat,values(8).toFloat,values(9).toFloat,values(10).toFloat,values(11).toFloat,values(12).toFloat,values(13).toFloat,values(14).toFloat,values(15).toBoolean,values(16))\n})\n\nval endTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"YYYY-MM-dd HH:mm:ss\"))\nval rddNewEndRecord = sc.parallelize(Array(Array(startTimestamp,endTimestamp,\"monthly overview\",market)))\nrddFundAudit = rddFundAudit.filter(line => line(0) != startTimestamp)\nrddFundAudit = rddFundAudit.union(rddNewEndRecord)\n\nrddFundAudit.map(line => line.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\nfs.delete(new Path(\"/tmp/audit/fund_audit_table\"),true)\nfs.rename(new Path(\"/tmp/tmp_file\"),new Path(\"/tmp/audit/fund_audit_table\"))\nrddFundAudit = sc.textFile(\"/tmp/audit/fund_audit_table/part*\").map(line => line.split(\";\"))",
      "user": "anonymous",
      "dateUpdated": "2021-07-04T23:09:53+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mopenClose\u001b[0m: \u001b[1m\u001b[32mBoolean\u001b[0m = false\n\u001b[1m\u001b[34mmarket\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = XAUUSD\n\u001b[1m\u001b[34mrddFundAudit\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[Array[String]]\u001b[0m = MapPartitionsRDD[1292] at map at <console>:269\n\u001b[1m\u001b[34mstartTimestamp\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2021-07-04 23:09:54\n\u001b[1m\u001b[34mrddNewFundAuditRecord\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[Array[String]]\u001b[0m = ParallelCollectionRDD[1242] at parallelize at <console>:101\nrddFundAudit: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[1292] at map at <console>:269\nrddFundAudit: org.apache.spark.rdd.RDD[Array[String]] = MapPartitionsRDD[1292] at map at <console>:269\n\u001b[1m\u001b[34mrddMonthly\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(String, String, Float, Float, Float, Float, Float, Float, Float, Float, Float...\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=290",
              "$$hashKey": "object:118877"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=291",
              "$$hashKey": "object:118878"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=292",
              "$$hashKey": "object:118879"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=293",
              "$$hashKey": "object:118880"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=294",
              "$$hashKey": "object:118881"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=295",
              "$$hashKey": "object:118882"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=296",
              "$$hashKey": "object:118883"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=297",
              "$$hashKey": "object:118884"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625131556909_1186129577",
      "id": "paragraph_1625037175665_2094177717",
      "dateCreated": "2021-07-01T09:25:56+0000",
      "dateStarted": "2021-07-04T23:09:53+0000",
      "dateFinished": "2021-07-04T23:09:57+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115792"
    },
    {
      "text": "%spark\n\nrddResult.collect()",
      "user": "anonymous",
      "dateUpdated": "2021-07-04T16:30:15+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres12\u001b[0m: \u001b[1m\u001b[32mArray[(String, String, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Boolean, String)]\u001b[0m = Array((XAUUSD,2012,0.2269995,0.0,0.0,0.0,0.0,0.0,0.0,0.9554004,0.0,0.27979982,0.0,-0.0047009275,1.4574988,false,2021-07-04 16:28:56), (XAUUSD,2013,0.4586011,0.0,0.0,0.0,1.9634998,0.0,0.0,-0.0029992675,0.0,-0.15206055,0.0,1.7177502,3.9847913,false,2021-07-04 16:28:56), (XAUUSD,2014,0.0,0.0,0.0,0.0,0.71017945,0.0,0.0,0.0,0.6851599,0.0,0.0,-0.022680664,1.3726587,false,2021-07-04 16:28:56), (XAUUSD,2015,-0.09207031,0.0,0.0,0.0,0.6062793,0.0,0.0,0.0,0.0,0.0,0.0,0.37697998,0.891189,false,2021-07-04 16:28:56), (XAUUSD,2016,-0.33465087,0.0,0.0,0.0,0.004499512,0.0,0.0,0.0,0.0,0.0,1.1126001,0.3465088,1.1289575,false,2021-07-04 16:28...\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=32",
              "$$hashKey": "object:118970"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625231785450_382135621",
      "id": "paragraph_1625231785450_382135621",
      "dateCreated": "2021-07-02T13:16:25+0000",
      "dateStarted": "2021-07-04T16:30:15+0000",
      "dateFinished": "2021-07-04T16:30:15+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115793"
    },
    {
      "text": "%spark\n\nrddFundAudit.collect()",
      "user": "anonymous",
      "dateUpdated": "2021-07-04T16:29:15+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres10\u001b[0m: \u001b[1m\u001b[32mArray[Array[String]]\u001b[0m = Array(Array(2021-07-04 16:28:56, null, monthly overview, XAUUSD), Array(2021-07-04 16:28:56, 2021-07-04 16:28:58, monthly overview, XAUUSD))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:34501/jobs/job?id=30",
              "$$hashKey": "object:119028"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625416147769_878919718",
      "id": "paragraph_1625416147769_878919718",
      "dateCreated": "2021-07-04T16:29:07+0000",
      "dateStarted": "2021-07-04T16:29:15+0000",
      "dateFinished": "2021-07-04T16:29:15+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115794"
    },
    {
      "text": "%spark\n\nrddTotal.collect()",
      "user": "anonymous",
      "dateUpdated": "2021-07-02T15:12:17+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres83\u001b[0m: \u001b[1m\u001b[32mArray[(String, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float)]\u001b[0m = Array((total,-0.013056718,0.0,0.0,0.0,0.005817519,0.0,1.8276396E-5,0.0010807747,0.013982856,-0.15043584,-9.1844166E-5,0.076355465,-0.06632952))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:36227/jobs/job?id=127",
              "$$hashKey": "object:119086"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625230978522_1458203302",
      "id": "paragraph_1625230978522_1458203302",
      "dateCreated": "2021-07-02T13:02:58+0000",
      "dateStarted": "2021-07-02T15:12:33+0000",
      "dateFinished": "2021-07-02T15:12:33+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115795"
    },
    {
      "text": "%spark\n\nrddPivotTable.collect()",
      "user": "anonymous",
      "dateUpdated": "2021-07-02T15:12:33+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres84\u001b[0m: \u001b[1m\u001b[32mArray[(String, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float)]\u001b[0m = Array((2016,0.0,0.0,0.0,0.0,0.70010984,0.0,0.0,0.0,0.0,0.0,1.1156299,0.0,1.8157398), (2017,0.115,0.0,0.0,0.0,0.0,0.0,2.1502001,0.0,0.0,0.8819897,0.0,0.0,3.1471899), (2018,0.035,0.0,0.0,0.0,0.49299073,0.0,0.0,0.0,-9.765625E-6,0.0,-1.7030005,-1.8509912,-3.0260108), (2012,0.2269995,0.0,0.0,0.0,0.0,0.0,0.0,0.9554004,0.0,0.2795996,0.0,-0.114100344,1.3478991), (2013,0.0,0.0,0.0,0.0,1.9634998,0.0,0.0,0.23421997,0.0,0.0,0.0,1.7177502,3.9154701), (2014,-6.958008E-5,0.0,0.0,0.0,0.0028295899,0.0,0.0,0.0,0.6851599,0.0,0.0,0.86489016,1.5528101), (2015,-0.09207031,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-1.0539185,0.0,0.37697998,-0.7690089))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:36227/jobs/job?id=128",
              "$$hashKey": "object:119144"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625133177351_1376555860",
      "id": "paragraph_1625133177351_1376555860",
      "dateCreated": "2021-07-01T09:52:57+0000",
      "dateStarted": "2021-07-02T15:12:33+0000",
      "dateFinished": "2021-07-02T15:12:33+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115796"
    },
    {
      "text": "%spark\n\nrddPrevClose.take(3)",
      "user": "anonymous",
      "dateUpdated": "2021-07-02T15:12:33+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres85\u001b[0m: \u001b[1m\u001b[32mArray[((Int, Int), ((java.util.Date, Float, Float, String), (java.util.Date, Float, Float, String)), Float)]\u001b[0m = Array(((2012,0),((Mon Jan 16 00:00:00 UTC 2012,1291.4,1291.4,XAUUSD),(Fri Jan 20 00:05:50 UTC 2012,1314.1,1314.1,XAUUSD)),1291.4), ((2012,7),((Thu Aug 16 15:15:20 UTC 2012,1409.75,1409.75,XAUUSD),(Thu Aug 16 15:19:30 UTC 2012,1409.64,1409.64,XAUUSD)),1314.1), ((2012,9),((Mon Oct 15 19:50:10 UTC 2012,1438.17,1438.17,XAUUSD),(Mon Oct 15 19:55:50 UTC 2012,1437.62,1437.6,XAUUSD)),1409.64))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:36227/jobs/job?id=129",
              "$$hashKey": "object:119202"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625131556909_645140368",
      "id": "paragraph_1625129159147_1352178662",
      "dateCreated": "2021-07-01T09:25:56+0000",
      "dateStarted": "2021-07-02T15:12:33+0000",
      "dateFinished": "2021-07-02T15:12:33+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115797"
    },
    {
      "text": "%spark\n",
      "user": "anonymous",
      "dateUpdated": "2021-07-02T15:12:33+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625131556910_1295542853",
      "id": "paragraph_1625129191144_359940997",
      "dateCreated": "2021-07-01T09:25:56+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115798"
    },
    {
      "text": "%spark\n\nrddTestGroup.filter(line => line._1==(2012,9)).take(10)",
      "user": "anonymous",
      "dateUpdated": "2021-07-02T15:12:33+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres86\u001b[0m: \u001b[1m\u001b[32mArray[((Int, Int), Iterable[(java.util.Date, Float, Float, String)])]\u001b[0m = Array(((2012,9),CompactBuffer((Mon Oct 15 19:50:10 UTC 2012,1438.17,1438.17,XAUUSD), (Mon Oct 15 19:50:20 UTC 2012,1438.45,1438.49,XAUUSD), (Mon Oct 15 19:50:30 UTC 2012,1438.61,1438.61,XAUUSD), (Mon Oct 15 19:50:40 UTC 2012,1438.65,1438.53,XAUUSD), (Mon Oct 15 19:50:50 UTC 2012,1438.52,1438.52,XAUUSD), (Mon Oct 15 19:51:00 UTC 2012,1438.51,1438.47,XAUUSD), (Mon Oct 15 19:51:10 UTC 2012,1438.48,1438.48,XAUUSD), (Mon Oct 15 19:51:20 UTC 2012,1438.46,1437.97,XAUUSD), (Mon Oct 15 19:51:30 UTC 2012,1437.9,1437.9,XAUUSD), (Mon Oct 15 19:51:40 UTC 2012,1437.95,1437.94,XAUUSD), (Mon Oct 15 19:51:50 UTC 2012,1437.88,1437.88,XAUUSD), (Mon Oct 15 19:52:00 UTC 2012,1437.87,1437.87,XAUUSD), (Mon...\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:36227/jobs/job?id=130",
              "$$hashKey": "object:119302"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:36227/jobs/job?id=131",
              "$$hashKey": "object:119303"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:36227/jobs/job?id=132",
              "$$hashKey": "object:119304"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625131556910_2108965547",
      "id": "paragraph_1625122209166_733601051",
      "dateCreated": "2021-07-01T09:25:56+0000",
      "dateStarted": "2021-07-02T15:12:33+0000",
      "dateFinished": "2021-07-02T15:12:34+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115799"
    },
    {
      "text": "%spark\n\nrddNewData.collect()",
      "user": "anonymous",
      "dateUpdated": "2021-07-04T16:09:53+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "<console>:35: \u001b[31merror: \u001b[0mnot found: value rddNewData\n       rddNewData.collect()\n       ^\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625131556910_1016604454",
      "id": "paragraph_1625119887649_543271398",
      "dateCreated": "2021-07-01T09:25:56+0000",
      "dateStarted": "2021-07-04T16:09:53+0000",
      "dateFinished": "2021-07-04T16:09:53+0000",
      "status": "ERROR",
      "$$hashKey": "object:115800"
    },
    {
      "text": "%spark\n\nrddMinInYear.take(3)",
      "user": "anonymous",
      "dateUpdated": "2021-07-02T15:12:34+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres88\u001b[0m: \u001b[1m\u001b[32mArray[((Int, Int), (java.util.Date, Float, Float, String))]\u001b[0m = Array(((2017,9),(Mon Oct 16 17:52:40 UTC 2017,2556.46,2556.46,XAUUSD)), ((2013,0),(Mon Jan 14 01:06:20 UTC 2013,1472.05,1472.05,XAUUSD)), ((2017,0),(Sun Jan 01 23:00:00 UTC 2017,2252.411,2252.411,XAUUSD)))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:36227/jobs/job?id=134",
              "$$hashKey": "object:119422"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625131556910_70719267",
      "id": "paragraph_1625120351388_801640216",
      "dateCreated": "2021-07-01T09:25:56+0000",
      "dateStarted": "2021-07-02T15:12:38+0000",
      "dateFinished": "2021-07-02T15:12:38+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115801"
    },
    {
      "text": "%spark\n\nrddMaxInYear.take(3)",
      "user": "anonymous",
      "dateUpdated": "2021-07-02T15:12:38+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres89\u001b[0m: \u001b[1m\u001b[32mArray[((Int, Int), (java.util.Date, Float, Float, String))]\u001b[0m = Array(((2017,9),(Mon Oct 16 17:58:00 UTC 2017,2555.63,2555.63,XAUUSD)), ((2013,0),(Mon Jan 14 01:18:30 UTC 2013,1472.05,1472.05,XAUUSD)), ((2017,0),(Sun Jan 01 23:05:50 UTC 2017,2252.411,2252.411,XAUUSD)))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:36227/jobs/job?id=135",
              "$$hashKey": "object:119480"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625131556911_1522530368",
      "id": "paragraph_1625121705942_1925449942",
      "dateCreated": "2021-07-01T09:25:56+0000",
      "dateStarted": "2021-07-02T15:12:38+0000",
      "dateFinished": "2021-07-02T15:12:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115802"
    },
    {
      "text": "%spark\n\nrddOpenClose.take(3)",
      "user": "anonymous",
      "dateUpdated": "2021-07-02T15:12:39+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres90\u001b[0m: \u001b[1m\u001b[32mArray[((Int, Int), ((java.util.Date, Float, Float, String), (java.util.Date, Float, Float, String)))]\u001b[0m = Array(((2012,0),((Mon Jan 16 00:00:00 UTC 2012,1291.4,1291.4,XAUUSD),(Fri Jan 20 00:05:50 UTC 2012,1314.1,1314.1,XAUUSD))), ((2012,7),((Thu Aug 16 15:15:20 UTC 2012,1409.75,1409.75,XAUUSD),(Thu Aug 16 15:19:30 UTC 2012,1409.64,1409.64,XAUUSD))), ((2012,9),((Mon Oct 15 19:50:10 UTC 2012,1438.17,1438.17,XAUUSD),(Mon Oct 15 19:55:50 UTC 2012,1437.62,1437.6,XAUUSD))))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:36227/jobs/job?id=136",
              "$$hashKey": "object:119538"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625131556911_223298125",
      "id": "paragraph_1625126278278_2113124099",
      "dateCreated": "2021-07-01T09:25:56+0000",
      "dateStarted": "2021-07-02T15:12:42+0000",
      "dateFinished": "2021-07-02T15:12:42+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115803"
    },
    {
      "text": "%spark\n\nrddPercents.collect()",
      "user": "anonymous",
      "dateUpdated": "2021-07-02T15:12:42+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres91\u001b[0m: \u001b[1m\u001b[32mArray[(Int, (Int, Float))]\u001b[0m = Array((2012,(0,0.2269995)), (2012,(7,0.9554004)), (2012,(9,0.2795996)), (2012,(11,-0.114100344)), (2013,(0,0.0)), (2013,(4,1.9634998)), (2013,(7,0.23421997)), (2013,(9,0.0)), (2013,(11,1.7177502)), (2014,(0,-6.958008E-5)), (2014,(4,0.0028295899)), (2014,(8,0.6851599)), (2014,(11,0.86489016)), (2015,(0,-0.09207031)), (2015,(4,0.0)), (2015,(9,-1.0539185)), (2015,(11,0.37697998)), (2016,(0,0.0)), (2016,(4,0.70010984)), (2016,(10,1.1156299)), (2016,(11,0.0)), (2017,(0,0.115)), (2017,(6,2.1502001)), (2017,(9,0.8819897)), (2017,(11,0.0)), (2018,(0,0.035)), (2018,(4,0.49299073)), (2018,(8,-9.765625E-6)), (2018,(10,-1.7030005)), (2018,(11,-1.8509912)))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:36227/jobs/job?id=137",
              "$$hashKey": "object:119596"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625131556911_1091920978",
      "id": "paragraph_1625126320524_1576470755",
      "dateCreated": "2021-07-01T09:25:56+0000",
      "dateStarted": "2021-07-02T15:12:42+0000",
      "dateFinished": "2021-07-02T15:12:42+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115804"
    },
    {
      "text": "%spark\n\n\nimport java.time.LocalDateTime\nimport java.time.format.DateTimeFormatter\nimport java.io.File\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{FileSystem, Path}\nimport java.security.{MessageDigest, DigestInputStream}\nimport java.nio.file.{Files, Paths}\n\ndef checkSumSha256(path: Path): String = {\n  val hdfs = FileSystem.get(new Configuration())\n\n  val buffer = new Array[Byte](8192)\n  val sha256 = MessageDigest.getInstance(\"SHA-256\")\n\n  val dis = new DigestInputStream(hdfs.open(path), sha256)\n  try {\n    while (dis.read(buffer) != -1) {}\n  }\n  finally {\n    dis.close()\n  }\n\n  sha256.digest.map(\"%02x\".format(_)).mkString\n}\n\ndef startAudit(filename: String, checkSum: String, startTimestamp: String, market: String) : Unit = {\n  val fs = FileSystem.get(new Configuration())\n  var rddLandingAudit = sc.textFile(\"/tmp/audit/landing_audit_table/part*\").map(line => line.split(\";\"))\n  val rddNewStartRecord = sc.parallelize(Array(Array(filename, checkSum, startTimestamp, market, \"load data from files\", \"landing_fund_db\", null, null)))\n  rddLandingAudit = rddLandingAudit.union(rddNewStartRecord)\n\n  rddLandingAudit.map(line => line.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\n  fs.delete(new Path(\"/tmp/audit/landing_audit_table\"), true)\n  fs.rename(new Path(\"/tmp/tmp_file\"), new Path(\"/tmp/audit/landing_audit_table\"))\n}\n\ndef endAudit(filename: String, checkSum: String, startTimestamp: String, market: String, count: String) : Unit = {\n  val fs = FileSystem.get(new Configuration())\n  val endTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"YYYY-MM-dd HH:mm:ss\"))\n  var rddLandingAudit = sc.textFile(\"/tmp/audit/landing_audit_table/part*\").map(line => line.split(\";\"))\n  val rddNewEndRecord = sc.parallelize(Array(Array(filename, checkSum, startTimestamp, market, \"load data from files\", \"landing_fund_db\", endTimestamp, count)))\n  rddLandingAudit = rddLandingAudit.filter(line => line(2) != startTimestamp)\n  rddLandingAudit = rddLandingAudit.union(rddNewEndRecord)\n\n  rddLandingAudit.map(line => line.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\n  fs.delete(new Path(\"/tmp/audit/landing_audit_table\"), true)\n  fs.rename(new Path(\"/tmp/tmp_file\"), new Path(\"/tmp/audit/landing_audit_table\"))\n}\n\ndef etlFromSourceToLanding(market: String, full: Boolean): Unit = {\n  var startTimestamp: String = null\n  val fs = FileSystem.get(new Configuration())\n  val files = fs.listStatus(new Path(\"/tmp/data\"))\n  var rddLandingFund: org.apache.spark.rdd.RDD[Array[String]] = null\n  var count: Long = 0L\n  var rddLandingAudit = sc.textFile(\"/tmp/audit/landing_audit_table/part*\").map(line => line.split(\";\"))\n\n  if (!full) {\n    rddLandingFund = sc.textFile(\"/tmp/landing_fund/landing_fund_table/part*\").map(line => line.split(\";\"))\n  }\n\n  for (file <- files) {\n    startTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\n    val checkSum = checkSumSha256(file.getPath)\n    val checkLog = rddLandingAudit.filter(line => line(1) == checkSum).count()\n\n    if (full || checkLog == 0) {\n      startAudit(file.getPath.toString, checkSum, startTimestamp, market)\n\n      if (rddLandingFund == null) {\n        rddLandingFund = sc.textFile(file.getPath.toString).map(line => line.split(\";\")).map(line => line ++ Array(startTimestamp, market))\n        count = rddLandingFund.count()\n      }\n      else {\n        val rddTemp = sc.textFile(file.getPath.toString).map(line => line.split(\";\")).map(line => line ++ Array(startTimestamp, market))\n        val yearMarketPairs = rddTemp.map(line => (line(0), line(6))).distinct().collect()\n        rddLandingFund = rddLandingFund.filter(line => !(yearMarketPairs contains(line(0), line(6))))\n        rddLandingFund = rddLandingFund.union(rddTemp)\n        count = rddTemp.count()\n      }\n      rddLandingFund.map(line => line.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\n      fs.delete(new Path(\"/tmp/landing_fund/landing_fund_table\"), true)\n      fs.rename(new Path(\"/tmp/tmp_file\"), new Path(\"/tmp/landing_fund/landing_fund_table\"))\n\n      endAudit(file.getPath.toString, checkSum, startTimestamp, market, count.toString)\n    }\n  }\n}\n\n//def main(args: Array[String]): Unit = {\n//  var MARKET: String = null\n//  var FULL: Boolean = false\n//  var INCREMENTAL: Boolean = false\n//  for (i <- args.indices) {\n//    args(i) match {\n//      case \"-m\" => {\n//        MARKET = args(i + 1)\n//      }\n//      case \"-i\" => {\n//        INCREMENTAL = true\n//      }\n//      case \"-f\"=> {\n//        FULL = true\n//      }\n//    }\n//  }\n//  if (FULL == INCREMENTAL) {\n//    throw new Exception(\"Choose only one load option\")\n//  }\netlFromSourceToLanding(\"XAUUSD\", false)\n//}\n",
      "user": "anonymous",
      "dateUpdated": "2021-07-05T14:34:42+0000",
      "config": {
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "colWidth": 12,
        "editorMode": "ace/mode/scala",
        "fontSize": 9,
        "results": {},
        "enabled": true
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import java.time.LocalDateTime\nimport java.time.format.DateTimeFormatter\nimport java.io.File\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{FileSystem, Path}\nimport java.security.{MessageDigest, DigestInputStream}\nimport java.nio.file.{Files, Paths}\n\u001b[1m\u001b[34mcheckSumSha256\u001b[0m: \u001b[1m\u001b[32m(path: org.apache.hadoop.fs.Path)String\u001b[0m\n\u001b[1m\u001b[34mstartAudit\u001b[0m: \u001b[1m\u001b[32m(filename: String, checkSum: String, startTimestamp: String, market: String)Unit\u001b[0m\n\u001b[1m\u001b[34mendAudit\u001b[0m: \u001b[1m\u001b[32m(filename: String, checkSum: String, startTimestamp: String, market: String, count: String)Unit\u001b[0m\n\u001b[1m\u001b[34metlFromSourceToLanding\u001b[0m: \u001b[1m\u001b[32m(market: String, full: Boolean)Unit\u001b[0m\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=0",
              "$$hashKey": "object:119654"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625131556911_954237138",
      "id": "paragraph_1625126723235_1027294572",
      "dateCreated": "2021-07-01T09:25:56+0000",
      "dateStarted": "2021-07-05T14:34:43+0000",
      "dateFinished": "2021-07-05T14:35:06+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115805"
    },
    {
      "text": "%spark\n\nsc.textFile(\"/tmp/stg_fund/stg_fund_table/part*\").collect()",
      "user": "anonymous",
      "dateUpdated": "2021-07-05T14:44:07+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres5\u001b[0m: \u001b[1m\u001b[32mArray[String]\u001b[0m = Array(2012-01-16 00:00:00;1291.4;1291.5;XAUUSD;2021-07-04 22:53:42, 2012-01-16 00:00:10;1291.4;1291.4;XAUUSD;2021-07-04 22:53:42, 2012-01-16 00:00:20;1291.6;1291.6;XAUUSD;2021-07-04 22:53:42, 2012-01-16 00:00:30;1291.5;1291.5;XAUUSD;2021-07-04 22:53:42, 2012-01-16 00:00:40;1291.1;1291.4;XAUUSD;2021-07-04 22:53:42, 2012-01-16 00:00:50;1291.4;1291.4;XAUUSD;2021-07-04 22:53:42, 2012-01-16 00:01:00;1291.1;1291.1;XAUUSD;2021-07-04 22:53:42, 2012-01-16 00:01:10;1290.9;1290.9;XAUUSD;2021-07-04 22:53:42, 2012-01-16 00:01:20;1291.1;1291.1;XAUUSD;2021-07-04 22:53:42, 2012-01-16 00:01:30;1290.9;1290.9;XAUUSD;2021-07-04 22:53:42, 2012-01-16 00:01:40;1290.9;1290.9;XAUUSD;2021-07-04 22:53:42, 2012-01-16 00:01:50;1290.6;1290.6;XAUUSD;2021-07-04 22:53:42,...\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=4",
              "$$hashKey": "object:119712"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625495683007_1199616283",
      "id": "paragraph_1625495683007_1199616283",
      "dateCreated": "2021-07-05T14:34:43+0000",
      "dateStarted": "2021-07-05T14:44:07+0000",
      "dateFinished": "2021-07-05T14:44:08+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115806"
    },
    {
      "text": "%spark\n\nimport java.time.LocalDateTime\nimport java.time.format.DateTimeFormatter\nimport java.io.File\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{FileSystem, Path}\nimport java.nio.file.{Files, Paths}\nimport java.text.SimpleDateFormat\nimport java.util.Date\n\ndef startAudit(startTimestamp: String): Unit = {\n  val fs = FileSystem.get(new Configuration())\n  var rddStgAudit = sc.textFile(\"/tmp/audit/stg_audit_table/part*\").map(line => line.split(\";\"))\n  val rddNewStartRecord = sc.parallelize(Array(Array(startTimestamp, null, \"load data from landing to staging\", 0.toString)))\n  rddStgAudit = rddStgAudit.union(rddNewStartRecord)\n\n  rddStgAudit.map(line => line.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\n  fs.delete(new Path(\"/tmp/audit/stg_audit_table\"), true)\n  fs.rename(new Path(\"/tmp/tmp_file\"), new Path(\"/tmp/audit/stg_audit_table\"))\n}\n\ndef endAudit(startTimestamp: String, count: String): Unit = {\n  val fs = FileSystem.get(new Configuration())\n  var rddStgAudit = sc.textFile(\"/tmp/audit/stg_audit_table/part*\").map(line => line.split(\";\"))\n  val endTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"YYYY-MM-dd HH:mm:ss\"))\n  val rddNewEndRecord = sc.parallelize(Array(Array(startTimestamp, endTimestamp, \"load data from landing to staging\", count)))\n  rddStgAudit = rddStgAudit.filter(line => line(0) != startTimestamp)\n  rddStgAudit = rddStgAudit.union(rddNewEndRecord)\n\n  rddStgAudit.map(line => line.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\n  fs.delete(new Path(\"/tmp/audit/stg_audit_table\"), true)\n  fs.rename(new Path(\"/tmp/tmp_file\"), new Path(\"/tmp/audit/stg_audit_table\"))\n  rddStgAudit = sc.textFile(\"/tmp/audit/stg_audit_table/part*\").map(line => line.split(\";\"))\n}\n\ndef etlFromLandingToStaging(full: Boolean): Unit = {\n  val rddStgAudit = sc.textFile(\"/tmp/audit/stg_audit_table/part*\").map(line => line.split(\";\"))\n  val recordCount = rddStgAudit.filter(line => line(1) != \"null\").count()\n  val fs = FileSystem.get(new Configuration())\n  val startTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\n  var lastStartDate: Date = null\n  if (recordCount != 0) {\n    val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n    lastStartDate = format.parse(rddStgAudit.filter(line => line(1) != \"null\").collect().last(1))\n  }\n  startAudit(startTimestamp)\n  val rddLandingFund = sc.textFile(\"/tmp/landing_fund/landing_fund_table/part*\").map(line => line.split(\";\"))\n  val rddLandingAudit = sc.textFile(\"/tmp/audit/landing_audit_table/part*\").map(line => line.split(\";\"))\n  val rddNewLandingFund = rddLandingFund.map(line => {\n    val market = line(7)\n    val loadDate = line(6)\n    val time = line(0)\n    val open = line(1)\n    val close = line(2)\n    ((market, loadDate), (time, open, close))\n  })\n  val rddPrepareToJoinAudit = rddLandingAudit.map(line => {\n    val startTime = line(2)\n    val market = line(3)\n    val endTime = line(6)\n    ((market, startTime), endTime)\n  })\n  var rddJoinedTimestamp = rddNewLandingFund.join(rddPrepareToJoinAudit).filter(line => {\n    val endTime = line._2._2\n    endTime != null\n  })\n  if (!full && recordCount != 0) {\n    rddJoinedTimestamp = rddJoinedTimestamp.filter(line => {\n      val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n      val startTime = format.parse(line._1._2)\n      startTime.after(lastStartDate)\n    })\n  }\n}\n\n//def main(args: Array[String]): Unit = {\n//  var FULL: Boolean = false\n//  var INCREMENTAL: Boolean = false\n//  for (i <- args.indices) {\n//    args(i) match {\n//      case \"-i\" || \"--incremental\" => {\n//        INCREMENTAL = true\n//      }\n//      case \"-f\" || \"--full\" => {\n//        FULL = true\n//      }\n//    }\n//  }\n//  if (FULL == INCREMENTAL) {\n//    throw new Exception(\"Choose only one load option\")\n//  }\netlFromLandingToStaging(false)\n//}",
      "user": "anonymous",
      "dateUpdated": "2021-07-05T18:41:25+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import java.time.LocalDateTime\nimport java.time.format.DateTimeFormatter\nimport java.io.File\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{FileSystem, Path}\nimport java.nio.file.{Files, Paths}\nimport java.text.SimpleDateFormat\nimport java.util.Date\n\u001b[1m\u001b[34mstartAudit\u001b[0m: \u001b[1m\u001b[32m(startTimestamp: String)Unit\u001b[0m\n\u001b[1m\u001b[34mendAudit\u001b[0m: \u001b[1m\u001b[32m(startTimestamp: String, count: String)Unit\u001b[0m\n\u001b[1m\u001b[34metlFromLandingToStaging\u001b[0m: \u001b[1m\u001b[32m(full: Boolean)Unit\u001b[0m\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=63",
              "$$hashKey": "object:119770"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=64",
              "$$hashKey": "object:119771"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=65",
              "$$hashKey": "object:119772"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625496181221_1852901695",
      "id": "paragraph_1625496181221_1852901695",
      "dateCreated": "2021-07-05T14:43:01+0000",
      "dateStarted": "2021-07-05T18:40:58+0000",
      "dateFinished": "2021-07-05T18:40:59+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115807"
    },
    {
      "text": "%spark\n\nrddJoinedTimestamp.collect()",
      "user": "anonymous",
      "dateUpdated": "2021-07-05T18:41:11+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres40\u001b[0m: \u001b[1m\u001b[32mArray[((String, String), ((String, String, String), String))]\u001b[0m = Array(((XAUUSD,2021-07-04 22:53:42),((Time (UTC),Open,High),2021-07-04 22:53:42)), ((XAUUSD,2021-07-04 22:53:42),((2012-01-16 00:00:00,1291,4,1291,5),2021-07-04 22:53:42)), ((XAUUSD,2021-07-04 22:53:42),((2012-01-16 00:00:10,1291,4,1291,4),2021-07-04 22:53:42)), ((XAUUSD,2021-07-04 22:53:42),((2012-01-16 00:00:20,1291,6,1291,6),2021-07-04 22:53:42)), ((XAUUSD,2021-07-04 22:53:42),((2012-01-16 00:00:30,1291,5,1291,5),2021-07-04 22:53:42)), ((XAUUSD,2021-07-04 22:53:42),((2012-01-16 00:00:40,1291,1,1291,4),2021-07-04 22:53:42)), ((XAUUSD,2021-07-04 22:53:42),((2012-01-16 00:00:50,1291,4,1291,4),2021-07-04 22:53:42)), ((XAUUSD,2021-07-04 22:53:42),((2012-01-16 00:01:00,1291,1,1291,1),2021-07-04...\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=66",
              "$$hashKey": "object:119838"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625510462993_1394188715",
      "id": "paragraph_1625510462993_1394188715",
      "dateCreated": "2021-07-05T18:41:02+0000",
      "dateStarted": "2021-07-05T18:41:11+0000",
      "dateFinished": "2021-07-05T18:41:12+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115808"
    },
    {
      "text": "%spark\n\nval full =false\n\nval rddStgAudit = sc.textFile(\"/tmp/audit/stg_audit_table/part*\").map(line => line.split(\";\"))\n  val recordCount = rddStgAudit.filter(line => line(1) != \"null\").count()\n  val fs = FileSystem.get(new Configuration())\n  val startTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\n  var lastStartDate: Date = null\n  if (recordCount != 0) {\n    val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n    lastStartDate = format.parse(rddStgAudit.filter(line => line(1) != \"null\").collect().last(1))\n  }\n  //startAudit(startTimestamp)\n  val rddLandingFund = sc.textFile(\"/tmp/landing_fund/landing_fund_table/part*\").map(line => line.split(\";\"))\n  val rddLandingAudit = sc.textFile(\"/tmp/audit/landing_audit_table/part*\").map(line => line.split(\";\"))\n  val rddNewLandingFund = rddLandingFund.map(line => {\n    val market = line(7)\n    val loadDate = line(6)\n    val time = line(0)\n    val open = line(1)\n    val close = line(2)\n    ((market, loadDate), (time, open, close))\n  })\n  val rddPrepareToJoinAudit = rddLandingAudit.map(line => {\n    val startTime = line(2)\n    val market = line(3)\n    val endTime = line(6)\n    ((market, startTime), endTime)\n  })\n  var rddJoinedTimestamp = rddNewLandingFund.join(rddPrepareToJoinAudit).filter(line => {\n    val endTime = line._2._2\n    endTime != null\n  })\n  if (!full && recordCount != 0) {\n    rddJoinedTimestamp = rddJoinedTimestamp.filter(line => {\n      val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n      val startTime = format.parse(line._1._1)\n      startTime.after(lastStartDate)\n    })\n  }\n  val rddClearFund = rddJoinedTimestamp.map(line => {\n    try {\n      val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n      val time = format.parse(line._2._1._1)\n      val open: Float = line._2._1._2.replace(\",\", \".\").toFloat\n      val close: Float = line._2._1._3.replace(\",\", \".\").toFloat\n      val market = line._1._1\n      val startTimestamp = format.parse(line._1._2)\n      (time, open, close, market, startTimestamp)\n    }\n    catch {\n      case e => {\n        (null, 0f, 0f, null, null)\n      }\n    }\n  })\n  val count = rddClearFund.count()\n  var rddStgFund : org.apache.spark.rdd.RDD[(java.util.Date, Float, Float, String, java.util.Date)] = null\n  val marketYearPairs = rddClearFund.map(line => (line._3, line._4)).distinct().collect()\n  if (full) {\n    rddStgFund = rddClearFund\n  }\n  else {\n    rddStgFund = sc.textFile(\"/tmp/stg_fund/stg_fund_table/part*\").map(line => {\n      try {\n        val values = line.split(\";\")\n        val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n        val time = format.parse(values(0))\n        val open: Float = values(1).toFloat\n        val close: Float = values(2).toFloat\n        val market = values(3)\n        val startTimestamp = format.parse(values(4))\n        (time, open, close, market, startTimestamp)\n      }\n      catch {\n        case e => {\n          (null, 0f, 0f, null, null)\n        }\n      }\n    }).filter(line => !(marketYearPairs contains(line._3, line._4)))\n    rddStgFund = rddStgFund.union(rddClearFund)\n  }\n  rddStgFund = rddStgFund.filter(line => line._1 != null && line._2 != 0 && line._3 != 0 && line._4 != null && line._5 != null)\n  rddStgFund.map(line => {\n    val dateFormat = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n    val date = dateFormat.format(line._1)\n    val startTimestamp = dateFormat.format(line._5)\n    (date, line._2, line._3, line._4, startTimestamp).productIterator.mkString(\";\")\n  }).saveAsTextFile(\"/tmp/tmp_file\")\n  fs.delete(new Path(\"/tmp/stg_fund/stg_fund_table\"), true)\n  fs.rename(new Path(\"/tmp/tmp_file\"), new Path(\"/tmp/stg_fund/stg_fund_table\"))\n  //endAudit(startTimestamp, count.toString)",
      "user": "anonymous",
      "dateUpdated": "2021-07-05T18:35:14+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "<console>:132: \u001b[33mwarning: \u001b[0mThis catches all Throwables. If this is really intended, use `case e : Throwable` to clear this warning.\n             case e => {\n                  ^\n<console>:156: \u001b[33mwarning: \u001b[0mThis catches all Throwables. If this is really intended, use `case e : Throwable` to clear this warning.\n               case e => {\n                    ^\norg.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 114.0 failed 1 times, most recent failure: Lost task 5.0 in stage 114.0 (TID 1662, localhost, executor driver): java.text.ParseException: Unparseable date: \"XAUUSD\"\n\tat java.text.DateFormat.parse(DateFormat.java:366)\n\tat $anonfun$new$2(<console>:117)\n\tat $anonfun$new$2$adapted(<console>:115)\n\tat scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1817)\n\tat org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1213)\n\tat org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1213)\n\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n\nDriver stacktrace:\n  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1926)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:1914)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:1913)\n  at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n  at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n  at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1913)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:948)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:948)\n  at scala.Option.foreach(Option.scala:407)\n  at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2147)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2096)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2085)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD.count(RDD.scala:1213)\n  ... 47 elided\nCaused by: java.text.ParseException: Unparseable date: \"XAUUSD\"\n  at java.text.DateFormat.parse(DateFormat.java:366)\n  at $anonfun$new$2(<console>:117)\n  at $anonfun$new$2$adapted(<console>:115)\n  at scala.collection.Iterator$$anon$12.hasNext(Iterator.scala:513)\n  at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\n  at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1817)\n  at org.apache.spark.rdd.RDD.$anonfun$count$1(RDD.scala:1213)\n  at org.apache.spark.rdd.RDD.$anonfun$count$1$adapted(RDD.scala:1213)\n  at org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2101)\n  at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n  at org.apache.spark.scheduler.Task.run(Task.scala:123)\n  at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:411)\n  at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n  at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\n  ... 3 more\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=49",
              "$$hashKey": "object:119896"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=50",
              "$$hashKey": "object:119897"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=51",
              "$$hashKey": "object:119898"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625496276628_322335737",
      "id": "paragraph_1625496276628_322335737",
      "dateCreated": "2021-07-05T14:44:36+0000",
      "dateStarted": "2021-07-05T18:35:15+0000",
      "dateFinished": "2021-07-05T18:35:16+0000",
      "status": "ERROR",
      "$$hashKey": "object:115809"
    },
    {
      "text": "%spark\nvar rddJoinedTimestamp = rddNewLandingFund.join(rddPrepareToJoinAudit).filter(line => {\n    val endTime = line._2._2\n    endTime != null\n  })\nrddPrepareToJoinAudit.collect()",
      "user": "anonymous",
      "dateUpdated": "2021-07-05T14:58:32+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mrddJoinedTimestamp\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[((String, String), ((String, String, String), String))]\u001b[0m = MapPartitionsRDD[81] at filter at <console>:45\n\u001b[1m\u001b[34mres22\u001b[0m: \u001b[1m\u001b[32mArray[((String, String), String)]\u001b[0m = Array(((XAUUSD,2021-07-04 22:53:42),2021-07-04 22:53:42))\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=23",
              "$$hashKey": "object:119964"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625496497765_763022135",
      "id": "paragraph_1625496497765_763022135",
      "dateCreated": "2021-07-05T14:48:17+0000",
      "dateStarted": "2021-07-05T14:58:32+0000",
      "dateFinished": "2021-07-05T14:58:33+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115810"
    },
    {
      "text": "%spark\n\nrddNewLandingFund.collect()",
      "user": "anonymous",
      "dateUpdated": "2021-07-05T14:52:24+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres18\u001b[0m: \u001b[1m\u001b[32mArray[((String, String), (String, String, String))]\u001b[0m = Array(((XAUUSD,2021-07-04 22:53:42),(Time (UTC),Open,High)), ((XAUUSD,2021-07-04 22:53:42),(2012-01-16 00:00:00,1291,4,1291,5)), ((XAUUSD,2021-07-04 22:53:42),(2012-01-16 00:00:10,1291,4,1291,4)), ((XAUUSD,2021-07-04 22:53:42),(2012-01-16 00:00:20,1291,6,1291,6)), ((XAUUSD,2021-07-04 22:53:42),(2012-01-16 00:00:30,1291,5,1291,5)), ((XAUUSD,2021-07-04 22:53:42),(2012-01-16 00:00:40,1291,1,1291,4)), ((XAUUSD,2021-07-04 22:53:42),(2012-01-16 00:00:50,1291,4,1291,4)), ((XAUUSD,2021-07-04 22:53:42),(2012-01-16 00:01:00,1291,1,1291,1)), ((XAUUSD,2021-07-04 22:53:42),(2012-01-16 00:01:10,1290,9,1290,9)), ((XAUUSD,2021-07-04 22:53:42),(2012-01-16 00:01:20,1291,1,1291,1)), ((XAUUSD,2021-07-04 22:53:42),(2012-0...\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=21",
              "$$hashKey": "object:120022"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625496658771_1070632151",
      "id": "paragraph_1625496658771_1070632151",
      "dateCreated": "2021-07-05T14:50:58+0000",
      "dateStarted": "2021-07-05T14:52:24+0000",
      "dateFinished": "2021-07-05T14:52:24+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115811"
    },
    {
      "text": "%spark\n\nlastStartDate",
      "user": "anonymous",
      "dateUpdated": "2021-07-05T14:55:35+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres21\u001b[0m: \u001b[1m\u001b[32mjava.util.Date\u001b[0m = Sun Jul 04 22:53:48 UTC 2021\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625496711291_1530783088",
      "id": "paragraph_1625496711291_1530783088",
      "dateCreated": "2021-07-05T14:51:51+0000",
      "dateStarted": "2021-07-05T14:55:35+0000",
      "dateFinished": "2021-07-05T14:55:35+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115812"
    },
    {
      "text": "%spark\n\nrddJoinedTimestamp.collect()",
      "user": "anonymous",
      "dateUpdated": "2021-07-05T14:58:50+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres23\u001b[0m: \u001b[1m\u001b[32mArray[((String, String), ((String, String, String), String))]\u001b[0m = Array(((XAUUSD,2021-07-04 22:53:42),((Time (UTC),Open,High),2021-07-04 22:53:42)), ((XAUUSD,2021-07-04 22:53:42),((2012-01-16 00:00:00,1291,4,1291,5),2021-07-04 22:53:42)), ((XAUUSD,2021-07-04 22:53:42),((2012-01-16 00:00:10,1291,4,1291,4),2021-07-04 22:53:42)), ((XAUUSD,2021-07-04 22:53:42),((2012-01-16 00:00:20,1291,6,1291,6),2021-07-04 22:53:42)), ((XAUUSD,2021-07-04 22:53:42),((2012-01-16 00:00:30,1291,5,1291,5),2021-07-04 22:53:42)), ((XAUUSD,2021-07-04 22:53:42),((2012-01-16 00:00:40,1291,1,1291,4),2021-07-04 22:53:42)), ((XAUUSD,2021-07-04 22:53:42),((2012-01-16 00:00:50,1291,4,1291,4),2021-07-04 22:53:42)), ((XAUUSD,2021-07-04 22:53:42),((2012-01-16 00:01:00,1291,1,1291,1),2021-07-04...\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=24",
              "$$hashKey": "object:120132"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625496869984_1993971851",
      "id": "paragraph_1625496869984_1993971851",
      "dateCreated": "2021-07-05T14:54:29+0000",
      "dateStarted": "2021-07-05T14:58:50+0000",
      "dateFinished": "2021-07-05T14:58:51+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115813"
    },
    {
      "text": "%spark\n\nval rddClearFund = rddJoinedTimestamp.map(line => {\n    try {\n      val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n      val time = format.parse(line._2._1._1)\n      val open: Float = line._2._1._2.replace(\",\", \".\").toFloat\n      val close: Float = line._2._1._3.replace(\",\", \".\").toFloat\n      val market = line._1._1\n      val startTimestamp = format.parse(line._1._2)\n      (time, open, close, market, startTimestamp)\n    }\n    catch {\n      case e => {\n        (null, 0f, 0f, null, null)\n      }\n    }\n  })",
      "user": "anonymous",
      "dateUpdated": "2021-07-05T14:59:30+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "<console>:57: \u001b[33mwarning: \u001b[0mThis catches all Throwables. If this is really intended, use `case e : Throwable` to clear this warning.\n             case e => {\n                  ^\n\u001b[1m\u001b[34mrddClearFund\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(java.util.Date, Float, Float, String, java.util.Date)]\u001b[0m = MapPartitionsRDD[82] at map at <console>:46\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625497130640_1994272558",
      "id": "paragraph_1625497130640_1994272558",
      "dateCreated": "2021-07-05T14:58:50+0000",
      "dateStarted": "2021-07-05T14:59:31+0000",
      "dateFinished": "2021-07-05T14:59:31+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115814"
    },
    {
      "text": "%spark\n\nrddClearFund.collect()",
      "user": "anonymous",
      "dateUpdated": "2021-07-05T14:59:42+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres24\u001b[0m: \u001b[1m\u001b[32mArray[(java.util.Date, Float, Float, String, java.util.Date)]\u001b[0m = Array((null,0.0,0.0,null,null), (Mon Jan 16 00:00:00 UTC 2012,1291.4,1291.5,XAUUSD,Sun Jul 04 22:53:42 UTC 2021), (Mon Jan 16 00:00:10 UTC 2012,1291.4,1291.4,XAUUSD,Sun Jul 04 22:53:42 UTC 2021), (Mon Jan 16 00:00:20 UTC 2012,1291.6,1291.6,XAUUSD,Sun Jul 04 22:53:42 UTC 2021), (Mon Jan 16 00:00:30 UTC 2012,1291.5,1291.5,XAUUSD,Sun Jul 04 22:53:42 UTC 2021), (Mon Jan 16 00:00:40 UTC 2012,1291.1,1291.4,XAUUSD,Sun Jul 04 22:53:42 UTC 2021), (Mon Jan 16 00:00:50 UTC 2012,1291.4,1291.4,XAUUSD,Sun Jul 04 22:53:42 UTC 2021), (Mon Jan 16 00:01:00 UTC 2012,1291.1,1291.1,XAUUSD,Sun Jul 04 22:53:42 UTC 2021), (Mon Jan 16 00:01:10 UTC 2012,1290.9,1290.9,XAUUSD,Sun Jul 04 22:53:42 UTC 2021), (Mon Jan 16 ...\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=25",
              "$$hashKey": "object:120242"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625497171213_1208096250",
      "id": "paragraph_1625497171213_1208096250",
      "dateCreated": "2021-07-05T14:59:31+0000",
      "dateStarted": "2021-07-05T14:59:42+0000",
      "dateFinished": "2021-07-05T14:59:42+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115815"
    },
    {
      "text": "%spark\n\nval count = rddClearFund.count()\n  var rddStgFund : org.apache.spark.rdd.RDD[(java.util.Date, Float, Float, String, java.util.Date)] = null\n  val marketYearPairs = rddClearFund.map(line => (line._3, line._4)).distinct().collect()\n  if (full) {\n    rddStgFund = rddClearFund\n  }\n  else {\n    rddStgFund = sc.textFile(\"/tmp/stg_fund/stg_fund_table/part*\").map(line => {\n      try {\n        val values = line.split(\";\")\n        val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n        val time = format.parse(values(0))\n        val open: Float = values(1).toFloat\n        val close: Float = values(2).toFloat\n        val market = values(3)\n        val startTimestamp = format.parse(values(4))\n        (time, open, close, market, startTimestamp)\n      }\n      catch {\n        case e => {\n          (null, 0f, 0f, null, null)\n        }\n      }\n    }).filter(line => !(marketYearPairs contains(line._3, line._4)))\n    rddStgFund = rddStgFund.union(rddClearFund)\n  }",
      "user": "anonymous",
      "dateUpdated": "2021-07-05T17:15:40+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "<console>:70: \u001b[33mwarning: \u001b[0mThis catches all Throwables. If this is really intended, use `case e : Throwable` to clear this warning.\n               case e => {\n                    ^\n\u001b[1m\u001b[34mcount\u001b[0m: \u001b[1m\u001b[32mLong\u001b[0m = 1446\n\u001b[1m\u001b[34mrddStgFund\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(java.util.Date, Float, Float, String, java.util.Date)]\u001b[0m = UnionRDD[91] at union at <console>:75\n\u001b[1m\u001b[34mmarketYearPairs\u001b[0m: \u001b[1m\u001b[32mArray[(Float, String)]\u001b[0m = Array((1409.83,XAUUSD), (0.0,null), (2205.083,XAUUSD), (2467.431,XAUUSD), (1290.6,XAUUSD), (2206.473,XAUUSD), (2736.331,XAUUSD), (2205.512,XAUUSD), (1296.4,XAUUSD), (1691.831,XAUUSD), (1437.62,XAUUSD), (1290.9,XAUUSD), (2206.509,XAUUSD), (2076.198,XAUUSD), (2906.331,XAUUSD), (1919.409,XAUUSD), (2466.631,XAUUSD), (1692.128,XAUUSD), (1437.51,XAUUSD), (1692.137,XAUUSD), (1409.87,XAUUSD), (2680.031,XAUUSD), (2094.18,XAUUSD), (2094.55,XAUUSD), (1409.67,XAUUSD), (1314.8,XAUUSD), (1409.65,XAUUSD), (2905.83,XAUUSD), (2206.512,X...\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=26",
              "$$hashKey": "object:120300"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=27",
              "$$hashKey": "object:120301"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625497182349_1594760425",
      "id": "paragraph_1625497182349_1594760425",
      "dateCreated": "2021-07-05T14:59:42+0000",
      "dateStarted": "2021-07-05T17:15:41+0000",
      "dateFinished": "2021-07-05T17:15:42+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115816"
    },
    {
      "text": "%spark\n\nrddStgFund = rddStgFund.filter(line => line._1 != null && line._2 != 0 && line._3 != 0 && line._4 != null && line._5 != null)\n  rddStgFund.map(line => {\n    val dateFormat = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n    val date = dateFormat.format(line._1)\n    val startTimestamp = dateFormat.format(line._5)\n    (date, line._2, line._3, line._4, startTimestamp).productIterator.mkString(\";\")\n  }).saveAsTextFile(\"/tmp/tmp_file\")\n  fs.delete(new Path(\"/tmp/stg_fund/stg_fund_table\"), true)\n  fs.rename(new Path(\"/tmp/tmp_file\"), new Path(\"/tmp/stg_fund/stg_fund_table\"))",
      "user": "anonymous",
      "dateUpdated": "2021-07-05T17:16:16+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "rddStgFund: org.apache.spark.rdd.RDD[(java.util.Date, Float, Float, String, java.util.Date)] = MapPartitionsRDD[92] at filter at <console>:47\n\u001b[1m\u001b[34mres26\u001b[0m: \u001b[1m\u001b[32mBoolean\u001b[0m = true\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=28",
              "$$hashKey": "object:120363"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625505341042_289313629",
      "id": "paragraph_1625505341042_289313629",
      "dateCreated": "2021-07-05T17:15:41+0000",
      "dateStarted": "2021-07-05T17:16:16+0000",
      "dateFinished": "2021-07-05T17:16:17+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115817"
    },
    {
      "text": "%spark\n\nsc.textFile(\"/tmp/stg_fund/stg_fund_table/part*\").collect()",
      "user": "anonymous",
      "dateUpdated": "2021-07-05T17:17:48+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres29\u001b[0m: \u001b[1m\u001b[32mArray[String]\u001b[0m = Array(2012-01-16 00:00:00;1291.4;1291.5;XAUUSD;2021-07-04 22:53:42, 2012-01-16 00:00:10;1291.4;1291.4;XAUUSD;2021-07-04 22:53:42, 2012-01-16 00:00:20;1291.6;1291.6;XAUUSD;2021-07-04 22:53:42, 2012-01-16 00:00:30;1291.5;1291.5;XAUUSD;2021-07-04 22:53:42, 2012-01-16 00:00:40;1291.1;1291.4;XAUUSD;2021-07-04 22:53:42, 2012-01-16 00:00:50;1291.4;1291.4;XAUUSD;2021-07-04 22:53:42, 2012-01-16 00:01:00;1291.1;1291.1;XAUUSD;2021-07-04 22:53:42, 2012-01-16 00:01:10;1290.9;1290.9;XAUUSD;2021-07-04 22:53:42, 2012-01-16 00:01:20;1291.1;1291.1;XAUUSD;2021-07-04 22:53:42, 2012-01-16 00:01:30;1290.9;1290.9;XAUUSD;2021-07-04 22:53:42, 2012-01-16 00:01:40;1290.9;1290.9;XAUUSD;2021-07-04 22:53:42, 2012-01-16 00:01:50;1290.6;1290.6;XAUUSD;2021-07-04 22:53:42...\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=30",
              "$$hashKey": "object:120421"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625505376871_671740630",
      "id": "paragraph_1625505376871_671740630",
      "dateCreated": "2021-07-05T17:16:16+0000",
      "dateStarted": "2021-07-05T17:17:48+0000",
      "dateFinished": "2021-07-05T17:17:49+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115818"
    },
    {
      "text": "%spark\n\nimport java.time.LocalDateTime\nimport java.time.format.DateTimeFormatter\nimport java.io.File\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{FileSystem, Path}\nimport java.nio.file.{Files, Paths}\nimport java.text.SimpleDateFormat\nimport java.util.Date\nimport java.util.Calendar\n\n\ndef etlFromStgToDestination(full: Boolean, market: String, openClose: Boolean): Unit = {\n  val startTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\n  val fs = FileSystem.get(new Configuration())\n  startAudit(startTimestamp, market)\n  var rddMonthly: org.apache.spark.rdd.RDD[(String, String, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Boolean, String)] = null\n  var rddStgFund = sc.textFile(\"/tmp/stg_fund/stg_fund_table/part*\").map(line => {\n    try {\n      val values = line.split(\";\")\n      val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n      val time = format.parse(values(0))\n      val open: Float = values(1).toFloat\n      val close: Float = values(2).toFloat\n      val market = values(3)\n      val startTimestamp = format.parse(values(4))\n      (time, open, close, market, startTimestamp)\n    }\n    catch {\n      case e => {\n        (null, 0f, 0f, null, null)\n      }\n    }\n  })\n  if (!full) {\n    rddMonthly = sc.textFile(\"/tmp/fund/monthly_table/part*\").map(line => {\n      val values = line.split(\";\")\n      (values(0), values(1), values(2).toFloat, values(3).toFloat, values(4).toFloat, values(5).toFloat,\n        values(6).toFloat, values(7).toFloat, values(8).toFloat, values(9).toFloat, values(10).toFloat,\n        values(11).toFloat, values(12).toFloat, values(13).toFloat, values(14).toFloat, values(15).toBoolean, values(16))\n    })\n    val dates = rddMonthly.filter(line => {\n      val monthlyMarket = line._1\n      val monthlyOpenClose = line._16\n      monthlyMarket == market && monthlyOpenClose == openClose\n    }).map(line => {\n      val format = new SimpleDateFormat(\"YYYY-MM-dd HH:mm:ss\")\n      val time = format.parse(line._17)\n      time\n    }).collect()\n    var lastDate: Date = null\n    if (!dates.isEmpty) {\n      lastDate = dates.reduceLeft((x, y) => {\n        if (x.after(y)) {\n          x\n        }\n        else {\n          y\n        }\n      })\n    }\n    val years = rddStgFund.filter(line => {\n      val stgMarket = line._4\n      val startTimestamp = line._5\n      stgMarket == market && (lastDate == null || startTimestamp.after(lastDate))\n    }).map(line => {\n      val cal = Calendar.getInstance\n      cal.setTime(line._1)\n      cal.get(Calendar.YEAR)\n    }).distinct().collect()\n    rddMonthly = rddMonthly.filter(line => ! {\n      val monthlyMarket = line._1\n      val year = line._2\n      (year == \"total\" || (years contains year.toInt)) && monthlyMarket == market\n    })\n    rddStgFund = rddStgFund.filter(line => {\n      val stgMarket = line._4\n      val cal = Calendar.getInstance\n      cal.setTime(line._1)\n      (years contains cal.get(Calendar.YEAR)) && stgMarket == market\n    })\n  }\n  else {\n    rddStgFund = rddStgFund.filter(line => {\n      val stgMarket = line._4\n      stgMarket == market\n    })\n  }\n  val rddNewData = rddStgFund.map(line => {\n    val cal = Calendar.getInstance\n    cal.setTime(line._1)\n    ((cal.get(Calendar.YEAR), cal.get(Calendar.MONTH)), (line._1, line._2, line._3, line._4))\n  })\n  val rddMinDateInMonth = rddNewData.reduceByKey((x, y) => {\n    if (x._1.before(y._1)) {\n      x\n    }\n    else {\n      y\n    }\n  })\n  val rddMaxDateInMonth = rddNewData.reduceByKey((x, y) => {\n    if (x._1.after(y._1)) {\n      x\n    }\n    else {\n      y\n    }\n  })\n  val rddOpenClose = rddMinDateInMonth.join(rddMaxDateInMonth).sortByKey()\n  var prevClose: Float = -1\n  val rddPrevClose = rddOpenClose.map(line => {\n    val temp = prevClose\n    prevClose = line._2._2._3\n    if (temp != -1) {\n      (line._1, line._2, temp)\n    }\n    else {\n      (line._1, line._2, line._2._1._2)\n    }\n  })\n  var rddPercents: org.apache.spark.rdd.RDD[(Int, (Int, Float))] = null\n  if (openClose) {\n    rddPercents = rddOpenClose.map(line => (line._1._1, (line._1._2, (line._2._2._3 - line._2._1._2) / line._2._1._2)))\n  }\n  else {\n    rddPercents = rddPrevClose.map(line => (line._1._1, (line._1._2, (line._2._2._3 - line._3) / line._3)))\n  }\n  val rddPivotTable = rddPercents.groupByKey().mapValues(value => value.toList).map(line => {\n    var jan, feb, march, april, may, june, july, aug, sep, oct, nov, dec, total: Float = 0\n    for (value <- line._2) {\n      value._1 match {\n        case 0 => jan += value._2\n        case 1 => feb += value._2\n        case 2 => march += value._2\n        case 3 => april += value._2\n        case 4 => may += value._2\n        case 5 => june += value._2\n        case 6 => july += value._2\n        case 7 => aug += value._2\n        case 8 => sep += value._2\n        case 9 => oct += value._2\n        case 10 => nov += value._2\n        case 11 => dec += value._2\n      }\n      total += value._2\n    }\n    (market, line._1.toString, jan, feb, march, april, may, june, july, aug, sep, oct, nov, dec, total, openClose, startTimestamp)\n  })\n  var rddResult = rddPivotTable\n  if (rddMonthly != null) {\n    rddResult = rddResult.union(rddMonthly)\n  }\n  val count = rddResult.filter(line => {\n    val monthlyMarket = line._1\n    val monthlyOpenClose = line._16\n    monthlyMarket == market && monthlyOpenClose == openClose\n  }).count()\n  val rddTotal = sc.parallelize(Array(rddResult.filter(line => {\n    val monthlyMarket = line._1\n    val monthlyOpenClose = line._16\n    monthlyMarket == market && monthlyOpenClose == openClose\n  }).reduce((x, y) => (market, \"total\", (x._3 + y._3) / count, (x._4 + y._4) / count, (x._5 + y._5) / count,\n    (x._6 + y._6) / count, (x._7 + y._7) / count, (x._8 + y._8) / count, (x._9 + y._9) / count, (x._10 + y._10) / count,\n    (x._11 + y._11) / count, (x._12 + y._12) / count, (x._13 + y._13) / count, (x._14 + y._14) / count,\n    (x._15 + y._15) / count, openClose, startTimestamp))))\n  rddResult = rddResult.union(rddTotal)\n  rddResult.map(line => line.productIterator.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\n  fs.delete(new Path(\"/tmp/fund/monthly_table\"), true)\n  fs.rename(new Path(\"/tmp/tmp_file\"), new Path(\"/tmp/fund/monthly_table\"))\n  endAudit(startTimestamp, market)\n}\n\ndef startAudit(startTimestamp: String, market: String): Unit = {\n  val fs = FileSystem.get(new Configuration())\n  var rddFundAudit = sc.textFile(\"/tmp/audit/fund_audit_table/part*\").map(line => line.split(\";\"))\n  val rddNewFundAuditRecord = sc.parallelize(Array(Array(startTimestamp, null, \"monthly overview\", market)))\n  rddFundAudit = rddFundAudit.union(rddNewFundAuditRecord)\n\n  rddFundAudit.map(line => line.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\n  fs.delete(new Path(\"/tmp/audit/fund_audit_table\"), true)\n  fs.rename(new Path(\"/tmp/tmp_file\"), new Path(\"/tmp/audit/fund_audit_table\"))\n  rddFundAudit = sc.textFile(\"/tmp/audit/fund_audit_table/part*\").map(line => line.split(\";\"))\n}\n\ndef endAudit(startTimestamp: String, market: String): Unit = {\n  val fs = FileSystem.get(new Configuration())\n  var rddFundAudit = sc.textFile(\"/tmp/audit/fund_audit_table/part*\").map(line => line.split(\";\"))\n  val endTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"YYYY-MM-dd HH:mm:ss\"))\n  val rddNewEndRecord = sc.parallelize(Array(Array(startTimestamp, endTimestamp, \"monthly overview\", market)))\n  rddFundAudit = rddFundAudit.filter(line => line(0) != startTimestamp)\n  rddFundAudit = rddFundAudit.union(rddNewEndRecord)\n\n  rddFundAudit.map(line => line.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\n  fs.delete(new Path(\"/tmp/audit/fund_audit_table\"), true)\n  fs.rename(new Path(\"/tmp/tmp_file\"), new Path(\"/tmp/audit/fund_audit_table\"))\n  rddFundAudit = sc.textFile(\"/tmp/audit/fund_audit_table/part*\").map(line => line.split(\";\"))\n}\n\n//def main(args: Array[String]): Unit = {\n//  var MARKET: String = null\n//  var FULL: Boolean = false\n//  var INCREMENTAL: Boolean = false\n//  var OPEN_CLOSE: Boolean = false\n//  var CLOSE_CLOSE: Boolean = false\n//  for (i <- args.indices) {\n//    args(i) match {\n//      case \"-m\" || \"--market\" => {\n//        MARKET = args(i + 1)\n//      }\n//      case \"-i\" || \"--incremental\" => {\n//        INCREMENTAL = true\n//      }\n//      case \"-f\" || \"--full\" => {\n//        FULL = true\n//      }\n//      case \"-oc\" || \"--open_close\" => {\n//        OPEN_CLOSE = true\n//      }\n//      case \"-cc\" || \"--close_close\" => {\n//        CLOSE_CLOSE = true\n//      }\n//    }\n//  }\n//  if (FULL == INCREMENTAL) {\n//    throw new Exception(\"Choose only one load option\")\n//  }\n//  if (OPEN_CLOSE == CLOSE_CLOSE) {\n//    throw new Exception(\"Choose only one previous value option\")\n//  }\netlFromStgToDestination(false, \"XAUUSD\", true)\n//}",
      "user": "anonymous",
      "dateUpdated": "2021-07-05T17:26:31+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "<console>:93: \u001b[33mwarning: \u001b[0mThis catches all Throwables. If this is really intended, use `case e : Throwable` to clear this warning.\n             case e => {\n                  ^\nimport java.time.LocalDateTime\nimport java.time.format.DateTimeFormatter\nimport java.io.File\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{FileSystem, Path}\nimport java.nio.file.{Files, Paths}\nimport java.text.SimpleDateFormat\nimport java.util.Date\nimport java.util.Calendar\n\u001b[1m\u001b[34metlFromStgToDestination\u001b[0m: \u001b[1m\u001b[32m(full: Boolean, market: String, openClose: Boolean)Unit\u001b[0m\n\u001b[1m\u001b[34mstartAudit\u001b[0m: \u001b[1m\u001b[32m(startTimestamp: String, market: String)Unit\u001b[0m\n\u001b[1m\u001b[34mendAudit\u001b[0m: \u001b[1m\u001b[32m(startTimestamp: String, market: String)Unit\u001b[0m\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=40",
              "$$hashKey": "object:120479"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=41",
              "$$hashKey": "object:120480"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=42",
              "$$hashKey": "object:120481"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=43",
              "$$hashKey": "object:120482"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=44",
              "$$hashKey": "object:120483"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=45",
              "$$hashKey": "object:120484"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=46",
              "$$hashKey": "object:120485"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=47",
              "$$hashKey": "object:120486"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625505432231_580411796",
      "id": "paragraph_1625505432231_580411796",
      "dateCreated": "2021-07-05T17:17:12+0000",
      "dateStarted": "2021-07-05T17:26:31+0000",
      "dateFinished": "2021-07-05T17:26:37+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115819"
    },
    {
      "text": "%spark\n\nsc.textFile(\"/tmp/fund/monthly_table/part*\").collect()",
      "user": "anonymous",
      "dateUpdated": "2021-07-05T17:26:39+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres34\u001b[0m: \u001b[1m\u001b[32mArray[String]\u001b[0m = Array(XAUUSD;2012;0.017577784;0.0;0.0;0.0;0.0;0.0;0.0;-7.801763E-5;0.0;-3.8246438E-4;0.0;-3.2950583E-4;0.016787797;true;2021-07-05 17:26:32, XAUUSD;2013;0.0;0.0;0.0;0.0;0.0;0.0;0.0;-1.772489E-4;0.0;0.0;0.0;0.022671647;0.022494398;true;2021-07-05 17:26:32, XAUUSD;2014;0.0;0.0;0.0;0.0;1.4744213E-4;0.0;0.0;0.0;-8.044136E-6;0.0;0.0;-0.0010921624;-9.527645E-4;true;2021-07-05 17:26:32, XAUUSD;2015;0.0;0.0;0.0;0.0;0.0;0.0;0.0;0.0;0.0;0.0;0.0;0.0;0.0;true;2021-07-05 17:26:32, XAUUSD;2016;0.0;0.0;0.0;0.0;2.1481997E-4;0.0;0.0;0.0;0.0;0.0;0.057819385;0.0;0.058034204;true;2021-07-05 17:26:32, XAUUSD;2017;0.0;0.0;0.0;0.0;0.0;0.0;3.2424982E-4;0.0;0.0;-3.2469828E-4;0.0;0.0;-4.4846092E-7;true;2021-07-05 17:26:32, XAUUSD;total;1.494087E-7;0.0;0.0;0.0;-8.1...\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=48",
              "$$hashKey": "object:120572"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625505815223_1653052179",
      "id": "paragraph_1625505815223_1653052179",
      "dateCreated": "2021-07-05T17:23:35+0000",
      "dateStarted": "2021-07-05T17:26:39+0000",
      "dateFinished": "2021-07-05T17:26:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115820"
    },
    {
      "text": "%spark\n\nval full =false\nval market = \"XAUUSD\"\nval openClose=false\n\nval startTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\n  val fs = FileSystem.get(new Configuration())\n  startAudit(startTimestamp, market)\n  var rddMonthly: org.apache.spark.rdd.RDD[(String, String, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Boolean, String)] = null\n  var rddStgFund = sc.textFile(\"/tmp/stg_fund/stg_fund_table/part*\").map(line => {\n    try {\n      val values = line.split(\";\")\n      val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n      val time = format.parse(values(0))\n      val open: Float = values(1).toFloat\n      val close: Float = values(2).toFloat\n      val market = values(3)\n      val startTimestamp = format.parse(values(4))\n      (time, open, close, market, startTimestamp)\n    }\n    catch {\n      case e => {\n        (null, 0f, 0f, null, null)\n      }\n    }\n  })\n  if (!full) {\n    \n    var rddMonthly = sc.textFile(\"/tmp/fund/monthly_table/part*\").map(line => {\n      val values = line.split(\";\")\n      (values(0), values(1), values(2).toFloat, values(3).toFloat, values(4).toFloat, values(5).toFloat,\n        values(6).toFloat, values(7).toFloat, values(8).toFloat, values(9).toFloat, values(10).toFloat,\n        values(11).toFloat, values(12).toFloat, values(13).toFloat, values(14).toFloat, values(15).toBoolean, values(16))\n    })\n    val dates = rddMonthly.filter(line => {\n      val monthlyMarket = line._1\n      val monthlyOpenClose = line._16\n      monthlyMarket == market && monthlyOpenClose == openClose\n    }).map(line => {\n      val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n      val time = format.parse(line._17)\n      time\n    }).collect()\n    var lastDate: Date = null\n    if (!dates.isEmpty) {\n      lastDate = dates.reduceLeft((x, y) => {\n        if (x.after(y)) {\n          x\n        }\n        else {\n          y\n        }\n      })\n    }\n    val years = rddStgFund.filter(line => {\n      val stgMarket = line._4\n      val startTimestamp = line._5\n      stgMarket == market && (lastDate == null || startTimestamp.after(lastDate))\n    }).map(line => {\n      val cal = Calendar.getInstance\n      cal.setTime(line._1)\n      cal.get(Calendar.YEAR)\n    }).distinct().collect()\n    rddMonthly = rddMonthly.filter(line => ! {\n      val monthlyMarket = line._1\n      val year = line._2\n      (year == \"total\" || (years contains year.toInt)) && monthlyMarket == market\n    })\n    rddStgFund = rddStgFund.filter(line => {\n      val stgMarket = line._4\n      val cal = Calendar.getInstance\n      cal.setTime(line._1)\n      (years contains cal.get(Calendar.YEAR)) && stgMarket == market\n    })\n  }\n  else {\n    rddStgFund = rddStgFund.filter(line => {\n      val stgMarket = line._4\n      stgMarket == market\n    })\n  }\n  val rddNewData = rddStgFund.map(line => {\n    val cal = Calendar.getInstance\n    cal.setTime(line._1)\n    ((cal.get(Calendar.YEAR), cal.get(Calendar.MONTH)), (line._1, line._2, line._3, line._4))\n  })\n  val rddMinDateInMonth = rddNewData.reduceByKey((x, y) => {\n    if (x._1.before(y._1)) {\n      x\n    }\n    else {\n      y\n    }\n  })\n  val rddMaxDateInMonth = rddNewData.reduceByKey((x, y) => {\n    if (x._1.after(y._1)) {\n      x\n    }\n    else {\n      y\n    }\n  })\n  val rddOpenClose = rddMinDateInMonth.join(rddMaxDateInMonth).sortByKey()\n  var prevClose: Float = -1\n  val rddPrevClose = rddOpenClose.map(line => {\n    val temp = prevClose\n    prevClose = line._2._2._3\n    if (temp != -1) {\n      (line._1, line._2, temp)\n    }\n    else {\n      (line._1, line._2, line._2._1._2)\n    }\n  })\n  var rddPercents: org.apache.spark.rdd.RDD[(Int, (Int, Float))] = null\n  if (openClose) {\n    rddPercents = rddOpenClose.map(line => (line._1._1, (line._1._2, (line._2._2._3 - line._2._1._2) / 100)))\n  }\n  else {\n    rddPercents = rddPrevClose.map(line => (line._1._1, (line._1._2, (line._2._2._3 - line._3) / 100)))\n  }\n  val rddPivotTable = rddPercents.groupByKey().mapValues(value => value.toList).map(line => {\n    var jan, feb, march, april, may, june, july, aug, sep, oct, nov, dec, total: Float = 0\n    for (value <- line._2) {\n      value._1 match {\n        case 0 => jan += value._2\n        case 1 => feb += value._2\n        case 2 => march += value._2\n        case 3 => april += value._2\n        case 4 => may += value._2\n        case 5 => june += value._2\n        case 6 => july += value._2\n        case 7 => aug += value._2\n        case 8 => sep += value._2\n        case 9 => oct += value._2\n        case 10 => nov += value._2\n        case 11 => dec += value._2\n      }\n      total += value._2\n    }\n    (market, line._1.toString, jan, feb, march, april, may, june, july, aug, sep, oct, nov, dec, total, openClose, startTimestamp)\n  })\n  var rddResult = rddPivotTable\n  if (rddMonthly != null) {\n    rddResult = rddResult.union(rddMonthly)\n  }\n  val count = rddResult.filter(line => {\n    val monthlyMarket = line._1\n    val monthlyOpenClose = line._16\n    monthlyMarket == market && monthlyOpenClose == openClose\n  }).count()\n  val rddTotal = sc.parallelize(Array(rddResult.filter(line => {\n    val monthlyMarket = line._1\n    val monthlyOpenClose = line._16\n    monthlyMarket == market && monthlyOpenClose == openClose\n  }).reduce((x, y) => (market, \"total\", (x._3 + y._3) / count, (x._4 + y._4) / count, (x._5 + y._5) / count,\n    (x._6 + y._6) / count, (x._7 + y._7) / count, (x._8 + y._8) / count, (x._9 + y._9) / count, (x._10 + y._10) / count,\n    (x._11 + y._11) / count, (x._12 + y._12) / count, (x._13 + y._13) / count, (x._14 + y._14) / count,\n    (x._15 + y._15) / count, openClose, startTimestamp))))\n  rddResult = rddResult.union(rddTotal)\n  rddResult.map(line => line.productIterator.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\n  fs.delete(new Path(\"/tmp/fund/monthly_table\"), true)\n  fs.rename(new Path(\"/tmp/tmp_file\"), new Path(\"/tmp/fund/monthly_table\"))\n  endAudit(startTimestamp, market)",
      "user": "anonymous",
      "dateUpdated": "2021-07-05T19:35:52+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "<console>:64: \u001b[31merror: \u001b[0mreassignment to val\n       full =false\n            ^\n<console>:65: \u001b[31merror: \u001b[0mnot found: value market\n       market = \"XAUUSD\"\n       ^\n<console>:66: \u001b[31merror: \u001b[0mnot found: value openClose\n       openClose=false\n       ^\n<console>:70: \u001b[31merror: \u001b[0mnot found: value market\n         startAudit(startTimestamp, market)\n                                    ^\n<console>:84: \u001b[33mwarning: \u001b[0mThis catches all Throwables. If this is really intended, use `case e : Throwable` to clear this warning.\n             case e => {\n                  ^\n<console>:99: \u001b[31merror: \u001b[0mnot found: value market\n             monthlyMarket == market && monthlyOpenClose == openClose\n                              ^\n<console>:99: \u001b[31merror: \u001b[0mnot found: value openClose\n             monthlyMarket == market && monthlyOpenClose == openClose\n                                                            ^\n<console>:119: \u001b[31merror: \u001b[0mnot found: value market\n             stgMarket == market && (lastDate == null || startTimestamp.after(lastDate))\n                          ^\n<console>:128: \u001b[31merror: \u001b[0mnot found: value market\n             (year == \"total\" || (years contains year.toInt)) && monthlyMarket == market\n                                                                                  ^\n<console>:140: \u001b[31merror: \u001b[0mnot found: value market\n             stgMarket == market\n                          ^\n<console>:177: \u001b[31merror: \u001b[0mnot found: value openClose\n         if (openClose) {\n             ^\n<console>:202: \u001b[31merror: \u001b[0mnot found: value market\n           (market, line._1.toString, jan, feb, march, april, may, june, july, aug, sep, oct, nov, dec, total, openClose, startTimestamp)\n            ^\n<console>:202: \u001b[31merror: \u001b[0mnot found: value openClose\n           (market, line._1.toString, jan, feb, march, april, may, june, july, aug, sep, oct, nov, dec, total, openClose, startTimestamp)\n                                                                                                               ^\n<console>:206: \u001b[31merror: \u001b[0mtype mismatch;\n found   : org.apache.spark.rdd.RDD[(String, String, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Boolean, String)]\n required: org.apache.spark.rdd.RDD[Nothing]\nNote: (String, String, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Float, Boolean, String) >: Nothing, but class RDD is invariant in type T.\nYou may wish to define T as -T instead. (SLS 4.5)\n           rddResult = rddResult.union(rddMonthly)\n                                       ^\n<console>:209: \u001b[31merror: \u001b[0mvalue _1 is not a member of Nothing\n           val monthlyMarket = line._1\n                                    ^\n<console>:210: \u001b[31merror: \u001b[0mvalue _16 is not a member of Nothing\n           val monthlyOpenClose = line._16\n                                       ^\n<console>:214: \u001b[31merror: \u001b[0mvalue _1 is not a member of Nothing\nError occurred in an application involving default arguments.\n           val monthlyMarket = line._1\n                                    ^\n<console>:215: \u001b[31merror: \u001b[0mvalue _16 is not a member of Nothing\nError occurred in an application involving default arguments.\n           val monthlyOpenClose = line._16\n                                       ^\n<console>:217: \u001b[31merror: \u001b[0mnot found: value market\nError occurred in an application involving default arguments.\n         }).reduce((x, y) => (market, \"total\", (x._3 + y._3) / count, (x._4 + y._4) / count, (x._5 + y._5) / count,\n                              ^\n<console>:220: \u001b[31merror: \u001b[0mnot found: value openClose\nError occurred in an application involving default arguments.\n           (x._15 + y._15) / count, openClose, startTimestamp))))\n                                    ^\n<console>:222: \u001b[31merror: \u001b[0mvalue productIterator is not a member of Nothing\n         rddResult.map(line => line.productIterator.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\n                                    ^\n<console>:227: \u001b[31merror: \u001b[0mnot found: value market\n         endAudit(startTimestamp, market)\n                                  ^\n<console>:229: \u001b[31merror: \u001b[0mnot found: value market\n       val $ires11 = market\n                     ^\n<console>:230: \u001b[31merror: \u001b[0mnot found: value openClose\n       val $ires12 = openClose\n                     ^\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625505864343_115346596",
      "id": "paragraph_1625505864343_115346596",
      "dateCreated": "2021-07-05T17:24:24+0000",
      "dateStarted": "2021-07-05T17:25:32+0000",
      "dateFinished": "2021-07-05T17:25:33+0000",
      "status": "ERROR",
      "$$hashKey": "object:115821"
    },
    {
      "text": "%spark\n\nsc.textFile(\"/tmp/data\").filter(line=>false).take(1).isEmpty",
      "user": "anonymous",
      "dateUpdated": "2021-07-07T12:05:48+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres46\u001b[0m: \u001b[1m\u001b[32mBoolean\u001b[0m = true\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=69",
              "$$hashKey": "object:120682"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=70",
              "$$hashKey": "object:120683"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625505932876_464493734",
      "id": "paragraph_1625505932876_464493734",
      "dateCreated": "2021-07-05T17:25:32+0000",
      "dateStarted": "2021-07-07T12:05:48+0000",
      "dateFinished": "2021-07-07T12:05:48+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115822"
    },
    {
      "text": "%spark\n\nimport scala.math.BigDecimal",
      "user": "anonymous",
      "dateUpdated": "2021-07-07T12:43:15+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import scala.math.BigDecimal\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625659455245_5282019",
      "id": "paragraph_1625659455245_5282019",
      "dateCreated": "2021-07-07T12:04:15+0000",
      "dateStarted": "2021-07-07T12:43:16+0000",
      "dateFinished": "2021-07-07T12:43:16+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115823"
    },
    {
      "text": "%spark\n\nvar a = BigDecimal(1.1)\na.precision",
      "user": "anonymous",
      "dateUpdated": "2021-07-07T12:50:03+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34ma\u001b[0m: \u001b[1m\u001b[32mscala.math.BigDecimal\u001b[0m = 1.1\n\u001b[1m\u001b[34mres53\u001b[0m: \u001b[1m\u001b[32mInt\u001b[0m = 2\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625661796202_1933917725",
      "id": "paragraph_1625661796202_1933917725",
      "dateCreated": "2021-07-07T12:43:16+0000",
      "dateStarted": "2021-07-07T12:50:03+0000",
      "dateFinished": "2021-07-07T12:50:03+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115824"
    },
    {
      "text": "import java.time.LocalDateTime\r\nimport java.time.format.DateTimeFormatter\r\nimport java.io.File\r\nimport org.apache.hadoop.conf.Configuration\r\nimport org.apache.hadoop.fs.{FileSystem, Path}\r\nimport java.security.{MessageDigest, DigestInputStream}\r\nimport java.nio.file.{Files, Paths}\r\nimport org.apache.spark.SparkContext._\r\nimport org.apache.spark.SparkContext\r\nimport org.apache.spark.sql.functions._\r\n\r\nobject LandingETLManager {\r\n\r\n  def checkSumSha256(path: Path): String = {\r\n    val hdfs = FileSystem.get(new Configuration())\r\n\r\n    val buffer = new Array[Byte](8192)\r\n    val sha256 = MessageDigest.getInstance(\"SHA-256\")\r\n\r\n    val dis = new DigestInputStream(hdfs.open(path), sha256)\r\n    try {\r\n      while (dis.read(buffer) != -1) {}\r\n    }\r\n    finally {\r\n      dis.close()\r\n    }\r\n\r\n    sha256.digest.map(\"%02x\".format(_)).mkString\r\n  }\r\n\r\n  def addRecordInAuditTable(sc: SparkContext,rddLandingAudit:org.apache.spark.rdd.RDD[Array[String]], newRecord:Array[String]): Unit = {\r\n    val fs = FileSystem.get(new Configuration())\r\n\r\n    val rddNewStartRecord = sc.parallelize(Array(newRecord))\r\n    rddLandingAudit = rddLandingAudit.union(rddNewStartRecord)\r\n\r\n    rddLandingAudit.map(line => line.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\r\n    fs.delete(new Path(\"/tmp/audit/landing_audit_table\"), true)\r\n    fs.rename(new Path(\"/tmp/tmp_file\"), new Path(\"/tmp/audit/landing_audit_table\"))\r\n  }\r\n\r\n\r\n  def etlFromSourceToLanding(sc: SparkContext, market: String, full: Boolean): Unit = {\r\n    var startTimestamp: String = null\r\n    val fs = FileSystem.get(new Configuration())\r\n    val files = fs.listStatus(new Path(\"/tmp/data/XAUUSD\"))\r\n    var rddLandingFund: org.apache.spark.rdd.RDD[Array[String]] = null\r\n    var count: Long = 0L\r\n    var rddLandingAudit: org.apache.spark.rdd.RDD[Array[String]] = null\r\n\r\n    if (fs.exists(new Path(\"/tmp/audit/landing_audit_table\"))) {\r\n      rddLandingAudit = sc.textFile(\"/tmp/audit/landing_audit_table/part*\").map(line => line.split(\";\"))\r\n    }\r\n    else {\r\n      rddLandingAudit = sc.emptyRDD[Array[String]]\r\n    }\r\n\r\n    if (!full) {\r\n      if (fs.exists(new Path(\"/tmp/audit/landing_audit_table\"))) {\r\n        rddLandingFund = sc.textFile(\"/tmp/landing_fund/landing_fund_table/part*\").map(line => line.split(\";\"))\r\n      }\r\n      else {\r\n        rddLandingFund = sc.emptyRDD[Array[String]]\r\n      }\r\n    }\r\n    \r\n    var newAuditRecord: Array[String] = null\r\n    \r\n    for (file <- files) {\r\n        try{\r\n      startTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\r\n      val checkSum = checkSumSha256(file.getPath)\r\n      val checkLog = rddLandingAudit.filter(line => line(1) == checkSum).take(1)\r\n\r\n      if (full || checkLog.isEmpty || (!checkLog.isEmpty && checkLog(6) == \"null\")) {\r\n          newAuditRecord = Array(file.getpath.toString, checkSum, startTimestamp, market, \"load data from files\", \"landing_fund_db\", null, null)\r\n        //startAudit(sc,rddLandingAudit file.getPath.toString, checkSum, startTimestamp, market)\r\n\r\n        if (rddLandingFund == null) {\r\n          rddLandingFund = sc.textFile(file.getPath.toString).map(line => line.split(\";\")).map(line => line ++ Array(startTimestamp, market))\r\n          count = rddLandingFund.count()\r\n        }\r\n        else {\r\n          val rddTemp = sc.textFile(file.getPath.toString).map(line => line.split(\";\")).map(line => line ++ Array(startTimestamp, market))\r\n          val yearMarketPairs = rddTemp.map(line => (line(0), line(6))).distinct().collect()\r\n          rddLandingFund = rddLandingFund.filter(line => !(yearMarketPairs contains(line(0), line(6))))\r\n          rddLandingFund = rddLandingFund.union(rddTemp)\r\n          count = rddTemp.count()\r\n        }\r\n        rddLandingFund.map(line => line.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\r\n        fs.delete(new Path(\"/tmp/landing_fund/landing_fund_table\"), true)\r\n        fs.rename(new Path(\"/tmp/tmp_file\"), new Path(\"/tmp/landing_fund/landing_fund_table\"))\r\n\r\nval endTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\r\n        //endAudit(sc, rddLanding_audit,file.getPath.toString, checkSum, startTimestamp, market, count.toString)\r\n        newAuditRecord = Array(file.getpath.toString, checkSum, startTimestamp, market, \"load data from files\", \"landing_fund_db\", endTimestamp, count)\r\n      }\r\n    }\r\n    catch{\r\n        case e => {\r\n            println(e)\r\n        }\r\n    }\r\n    finally{\r\n        addRecordInAuditTable(sc,rddLandingAudit,newRecord)\r\n    }\r\n    }\r\n  }\r\n}\r\n",
      "user": "anonymous",
      "dateUpdated": "2021-07-12T09:37:17+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "<console>:333: \u001b[33mwarning: \u001b[0mcomparing values of types Array[String] and String using `==' will always yield false\n             if (full || checkLog.isEmpty || (!checkLog.isEmpty && checkLog(6) == \"null\")) {\n                                                                               ^\nimport java.time.LocalDateTime\nimport java.time.format.DateTimeFormatter\nimport java.io.File\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{FileSystem, Path}\nimport java.security.{MessageDigest, DigestInputStream}\nimport java.nio.file.{Files, Paths}\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkContext\nimport org.apache.spark.sql.functions._\ndefined object LandingETLManager\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625661807690_1247633814",
      "id": "paragraph_1625661807690_1247633814",
      "dateCreated": "2021-07-07T12:43:27+0000",
      "dateStarted": "2021-07-08T08:37:31+0000",
      "dateFinished": "2021-07-08T08:37:32+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115825"
    },
    {
      "text": "%spark\n\nLandingETLManager.etlFromSourceToLanding(sc,\"XAUUSD\",true)",
      "user": "anonymous",
      "dateUpdated": "2021-07-08T08:37:38+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "org.apache.spark.SparkException: Job 110 cancelled part of cancelled job group zeppelin|anonymous|2G95VUHBF|paragraph_1625662335126_1608446037\n  at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:1926)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1861)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleJobGroupCancelled$4(DAGScheduler.scala:929)\n  at scala.runtime.java8.JFunction1$mcVI$sp.apply(JFunction1$mcVI$sp.java:23)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n  at org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:928)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2116)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2096)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2085)\n  at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n  at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n  at org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n  at org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:990)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:385)\n  at org.apache.spark.rdd.RDD.collect(RDD.scala:989)\n  at LandingETLManager$.$anonfun$etlFromSourceToLanding$3(<console>:342)\n  at LandingETLManager$.$anonfun$etlFromSourceToLanding$3$adapted(<console>:328)\n  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n  at LandingETLManager$.etlFromSourceToLanding(<console>:328)\n  ... 91 elided\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=99",
              "$$hashKey": "object:120901"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=100",
              "$$hashKey": "object:120902"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=101",
              "$$hashKey": "object:120903"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=102",
              "$$hashKey": "object:120904"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=103",
              "$$hashKey": "object:120905"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=104",
              "$$hashKey": "object:120906"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=105",
              "$$hashKey": "object:120907"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=106",
              "$$hashKey": "object:120908"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=107",
              "$$hashKey": "object:120909"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=108",
              "$$hashKey": "object:120910"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=109",
              "$$hashKey": "object:120911"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=110",
              "$$hashKey": "object:120912"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625662335126_1608446037",
      "id": "paragraph_1625662335126_1608446037",
      "dateCreated": "2021-07-07T12:52:15+0000",
      "dateStarted": "2021-07-08T08:37:38+0000",
      "dateFinished": "2021-07-08T12:32:48+0000",
      "status": "ABORT",
      "$$hashKey": "object:115826"
    },
    {
      "text": "%spark\n\nsc.textFile(\"/tmp/landing_fund/landing_fund_table/part*\").collect()",
      "user": "anonymous",
      "dateUpdated": "2021-07-07T14:33:29+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres58\u001b[0m: \u001b[1m\u001b[32mArray[String]\u001b[0m = Array(Time (UTC);Open;High;Low;Close;Volume ;2021-07-07 14:33:15;XAUUSD, 2012-01-16 00:00:00;1291,4;1291,5;1291,4;1291,4;0;2021-07-07 14:33:15;XAUUSD, 2012-01-16 00:00:10;1291,4;1291,4;1291,4;1291,4;0;2021-07-07 14:33:15;XAUUSD, 2012-01-16 00:00:20;1291,6;1291,6;1291,4;1291,4;0;2021-07-07 14:33:15;XAUUSD, 2012-01-16 00:00:30;1291,5;1291,5;1291,4;1291,4;0;2021-07-07 14:33:15;XAUUSD, 2012-01-16 00:00:40;1291,1;1291,4;1291,1;1291,1;0;2021-07-07 14:33:15;XAUUSD, 2012-01-16 00:00:50;1291,4;1291,4;1291,1;1291,1;0;2021-07-07 14:33:15;XAUUSD, 2012-01-16 00:01:00;1291,1;1291,1;1291,1;1291,1;0;2021-07-07 14:33:15;XAUUSD, 2012-01-16 00:01:10;1290,9;1290,9;1290,9;1290,9;0;2021-07-07 14:33:15;XAUUSD, 2012-01-16 00:01:20;1291,1;1291,1;1291,1;1291,1;0;2...\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=81",
              "$$hashKey": "object:121014"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625666358911_1689552367",
      "id": "paragraph_1625666358911_1689552367",
      "dateCreated": "2021-07-07T13:59:18+0000",
      "dateStarted": "2021-07-07T14:33:29+0000",
      "dateFinished": "2021-07-07T14:33:30+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115827"
    },
    {
      "text": "%spark\n\nimport java.time.LocalDateTime\nimport java.time.format.DateTimeFormatter\nimport java.io.File\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{FileSystem, Path}\nimport java.nio.file.{Files, Paths}\nimport java.text.SimpleDateFormat\nimport java.util.Date\nimport scala.math.BigDecimal\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkContext\nimport org.apache.spark.sql.functions._\n\nobject StgETLManager {\n  def startAudit(sc: SparkContext, startTimestamp: String): Unit = {\n    val fs = FileSystem.get(new Configuration())\n    var rddStgAudit: org.apache.spark.rdd.RDD[Array[String]] = null\n    if (fs.exists(new Path(\"/tmp/audit/stg_audit_table\"))) {\n      rddStgAudit = sc.textFile(\"/tmp/audit/stg_audit_table/part*\").map(line => line.split(\";\"))\n    }\n    else {\n      rddStgAudit = sc.emptyRDD[Array[String]]\n    }\n\n    val rddNewStartRecord = sc.parallelize(Array(Array(startTimestamp, null, \"load data from landing to staging\", 0.toString)))\n    rddStgAudit = rddStgAudit.union(rddNewStartRecord)\n\n    rddStgAudit.map(line => line.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\n    fs.delete(new Path(\"/tmp/audit/stg_audit_table\"), true)\n    fs.rename(new Path(\"/tmp/tmp_file\"), new Path(\"/tmp/audit/stg_audit_table\"))\n  }\n\n  def endAudit(sc: SparkContext, startTimestamp: String, count: String): Unit = {\n    val fs = FileSystem.get(new Configuration())\n    var rddStgAudit = sc.textFile(\"/tmp/audit/stg_audit_table/part*\").map(line => line.split(\";\"))\n    val endTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\n    val rddNewEndRecord = sc.parallelize(Array(Array(startTimestamp, endTimestamp, \"load data from landing to staging\", count)))\n    rddStgAudit = rddStgAudit.filter(line => line(0) != startTimestamp)\n    rddStgAudit = rddStgAudit.union(rddNewEndRecord)\n\n    rddStgAudit.map(line => line.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\n    fs.delete(new Path(\"/tmp/audit/stg_audit_table\"), true)\n    fs.rename(new Path(\"/tmp/tmp_file\"), new Path(\"/tmp/audit/stg_audit_table\"))\n  }\n\n  def etlFromLandingToStaging(sc: SparkContext, full: Boolean): Unit = {\n    val fs = FileSystem.get(new Configuration())\n\n    var rddStgAudit: org.apache.spark.rdd.RDD[Array[String]] = null\n    if (fs.exists(new Path(\"/tmp/audit/stg_audit_table\"))) {\n      rddStgAudit = sc.textFile(\"/tmp/audit/stg_audit_table/part*\").map(line => line.split(\";\"))\n    }\n    else {\n      rddStgAudit = sc.emptyRDD[Array[String]]\n    }\n\n    val recordCount = rddStgAudit.filter(line => line(1) != \"null\").count()\n\n    val startTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\n    var lastStartDate: Date = null\n\n    if (recordCount != 0) {\n      val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n      lastStartDate = format.parse(rddStgAudit.filter(line => line(1) != \"null\").collect().last(1))\n    }\n\n    startAudit(sc, startTimestamp)\n\n    var rddLandingFund: org.apache.spark.rdd.RDD[Array[String]] = null\n    if (fs.exists(new Path(\"/tmp/audit/landing_audit_table\"))) {\n      rddLandingFund = sc.textFile(\"/tmp/landing_fund/landing_fund_table/part*\").map(line => line.split(\";\"))\n    }\n    else {\n      rddLandingFund = sc.emptyRDD[Array[String]]\n    }\n\n    var rddLandingAudit: org.apache.spark.rdd.RDD[Array[String]] = null\n    if (fs.exists(new Path(\"/tmp/audit/landing_audit_table\"))) {\n      rddLandingAudit = sc.textFile(\"/tmp/audit/landing_audit_table/part*\").map(line => line.split(\";\"))\n    }\n    else {\n      rddLandingAudit = sc.emptyRDD[Array[String]]\n    }\n\n    val rddNewLandingFund = rddLandingFund.map(line => {\n      val market = line(7)\n      val loadDate = line(6)\n      val time = line(0)\n      val open = line(1)\n      val close = line(2)\n      ((market, loadDate), (time, open, close))\n    })\n    val rddPrepareToJoinAudit = rddLandingAudit.map(line => {\n      val startTime = line(2)\n      val market = line(3)\n      val endTime = line(6)\n      ((market, startTime), endTime)\n    })\n    var rddJoinedTimestamp = rddNewLandingFund.join(rddPrepareToJoinAudit).filter(line => {\n      val endTime = line._2._2\n      endTime != null\n    })\n\n    if (!full && recordCount != 0) {\n      rddJoinedTimestamp = rddJoinedTimestamp.filter(line => {\n        val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n        val startTime = format.parse(line._1._2)\n        startTime.after(lastStartDate)\n      })\n    }\n    val rddClearFund = rddJoinedTimestamp.map(line => {\n      try {\n        val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n        val time = format.parse(line._2._1._1)\n        val open = BigDecimal(line._2._1._2.replace(\",\", \".\"))\n        val close = BigDecimal(line._2._1._3.replace(\",\", \".\"))\n        val market = line._1._1\n        val startTimestamp = format.parse(line._1._2)\n        (time, open, close, market, startTimestamp)\n      }\n      catch {\n        case e => {\n          (null, BigDecimal(0), BigDecimal(0), null, null)\n        }\n      }\n    })\n\n    val count = rddClearFund.count()\n    var rddStgFund: org.apache.spark.rdd.RDD[(java.util.Date, scala.math.BigDecimal, scala.math.BigDecimal, String, java.util.Date)] = null\n    val marketYearPairs = rddClearFund.map(line => (line._3, line._4)).distinct().collect()\n\n    if (full) {\n      rddStgFund = rddClearFund\n    }\n    else {\n      if (fs.exists(new Path(\"/tmp/stg_fund/stg_fund_table\"))) {\n        rddStgFund = sc.textFile(\"/tmp/stg_fund/stg_fund_table/part*\").map(line => {\n          try {\n            val values = line.split(\";\")\n            val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n            val time = format.parse(values(0))\n            val open = BigDecimal(values(1))\n            val close = BigDecimal(values(2))\n            val market = values(3)\n            val startTimestamp = format.parse(values(4))\n            (time, open, close, market, startTimestamp)\n          }\n          catch {\n            case e => {\n              (null, BigDecimal(0), BigDecimal(0), null, null)\n            }\n          }\n        }).filter(line => !(marketYearPairs contains(line._3, line._4)))\n      }\n      else {\n        rddStgFund = sc.emptyRDD[(java.util.Date, scala.math.BigDecimal, scala.math.BigDecimal, String, java.util.Date)]\n      }\n\n\n      rddStgFund = rddStgFund.union(rddClearFund)\n    }\n\n    rddStgFund = rddStgFund.filter(line => line._1 != null && line._2 != 0 && line._3 != 0 && line._4 != null && line._5 != null)\n    rddStgFund.map(line => {\n      val dateFormat = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n      val date = dateFormat.format(line._1)\n      val startTimestamp = dateFormat.format(line._5)\n      (date, line._2, line._3, line._4, startTimestamp).productIterator.mkString(\";\")\n    }).saveAsTextFile(\"/tmp/tmp_file\")\n\n    fs.delete(new Path(\"/tmp/stg_fund/stg_fund_table\"), true)\n    fs.rename(new Path(\"/tmp/tmp_file\"), new Path(\"/tmp/stg_fund/stg_fund_table\"))\n\n    endAudit(sc, startTimestamp, count.toString)\n  }\n}",
      "user": "anonymous",
      "dateUpdated": "2021-07-07T15:28:38+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "<console>:306: \u001b[33mwarning: \u001b[0mThis catches all Throwables. If this is really intended, use `case e : Throwable` to clear this warning.\n               case e => {\n                    ^\n<console>:333: \u001b[33mwarning: \u001b[0mThis catches all Throwables. If this is really intended, use `case e : Throwable` to clear this warning.\n                   case e => {\n                        ^\nimport java.time.LocalDateTime\nimport java.time.format.DateTimeFormatter\nimport java.io.File\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{FileSystem, Path}\nimport java.nio.file.{Files, Paths}\nimport java.text.SimpleDateFormat\nimport java.util.Date\nimport scala.math.BigDecimal\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkContext\nimport org.apache.spark.sql.functions._\ndefined object StgETLManager\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625666483156_486142909",
      "id": "paragraph_1625666483156_486142909",
      "dateCreated": "2021-07-07T14:01:23+0000",
      "dateStarted": "2021-07-07T15:28:39+0000",
      "dateFinished": "2021-07-07T15:28:39+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115828"
    },
    {
      "text": "%spark\n\nStgETLManager.etlFromLandingToStaging(sc,true)",
      "user": "anonymous",
      "dateUpdated": "2021-07-07T14:36:03+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=82",
              "$$hashKey": "object:121114"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=83",
              "$$hashKey": "object:121115"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=84",
              "$$hashKey": "object:121116"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=85",
              "$$hashKey": "object:121117"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=86",
              "$$hashKey": "object:121118"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=87",
              "$$hashKey": "object:121119"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=88",
              "$$hashKey": "object:121120"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625668529078_1156775047",
      "id": "paragraph_1625668529078_1156775047",
      "dateCreated": "2021-07-07T14:35:29+0000",
      "dateStarted": "2021-07-07T14:36:03+0000",
      "dateFinished": "2021-07-07T14:36:07+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115829"
    },
    {
      "text": "%spark\n\n\nimport java.time.LocalDateTime\nimport java.time.format.DateTimeFormatter\nimport java.io.File\n\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{FileSystem, Path}\nimport java.nio.file.{Files, Paths}\nimport java.text.SimpleDateFormat\nimport java.util.Date\nimport java.util.Calendar\n\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkContext\nimport org.apache.spark.sql.functions._\n\nimport scala.math.BigDecimal\n\nobject DestinationETLManager {\n  def addRecordInAuditTable(sc: SparkContext, rddFundAudit: org.apache.spark.rdd.RDD[Array[String]], newRecord: Array[String]): Unit = {\n    val fs = FileSystem.get(new Configuration())\n\n    val rddNewStartRecord = sc.parallelize(Array(newRecord))\n    rddFundAudit.union(rddNewStartRecord).map(line => line.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_audit_file\")\n\n    fs.delete(new Path(\"/tmp/audit/fund_audit_table\"), true)\n    fs.rename(new Path(\"/tmp/tmp_audit_file\"), new Path(\"/tmp/audit/fund_audit_table\"))\n    //fs.delete(new Path(\"/tmp/tmp_audit_file\"), true)\n  }\n\n  def findLastUpdateDate(rddFundAudit: org.apache.spark.rdd.RDD[Array[String]]): Date = {\n    val dates = rddFundAudit.filter(line => line(1) != \"null\").map(line => {\n      val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n      val startTimestamp = format.parse(line(0))\n      startTimestamp\n    }).collect()\n\n    var lastDate: Date = null\n    if (!dates.isEmpty) {\n      lastDate = dates.reduceLeft((x, y) => {\n        if (x.after(y)) {\n          x\n        }\n        else {\n          y\n        }\n      })\n    }\n    lastDate\n  }\n\n  def getSuccessfulUploadedData(rddStgFund: org.apache.spark.rdd.RDD[(java.util.Date, scala.math.BigDecimal, scala.math.BigDecimal, String, java.util.Date)],\n                                rddStgAudit: org.apache.spark.rdd.RDD[Array[String]]):\n  org.apache.spark.rdd.RDD[(java.util.Date, scala.math.BigDecimal, scala.math.BigDecimal, String, java.util.Date)] = {\n    //(time, open, close, market, startTimestamp)\n    val rddPrepareToJoinFund = rddStgFund.map(line => {\n      val (time, open, close, market, loadDate) = line\n      ((market, loadDate), (time, open, close))\n    })\n    val rddPrepareToJoinAudit = rddStgAudit.map(line => {\n        try{\n             val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n      val startTime = format.parse(line(0))\n      val market = line(3)\n      val endTime = format.parse(line(1))\n      ((market, startTime), endTime)\n        }\n        catch {\n            case _ => {\n                ((null,null),null)\n            }\n        }\n    })\n    var rddCheckEndDate = rddPrepareToJoinFund.join(rddPrepareToJoinAudit).filter(line => {\n      val endTime = line._2._2\n      endTime != null\n    }).map(line => {\n      val ((market, startTime), ((time, open, close), endTime)) = line\n      (time, open, close, market, startTime)\n    })\n\n    rddCheckEndDate\n  }\n\n  def getDataToCalculate(rddStgFund: org.apache.spark.rdd.RDD[(java.util.Date, scala.math.BigDecimal, scala.math.BigDecimal, String, java.util.Date)],\n                         rddMonthly: org.apache.spark.rdd.RDD[(String, String, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal,\n                           BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, Boolean, String)],\n                         lastUpdateDate: Date, full: Boolean, market: String):\n  (org.apache.spark.rdd.RDD[((Int, Int, String), (Date, BigDecimal, BigDecimal))],\n    org.apache.spark.rdd.RDD[(String, String, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal,\n      BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, Boolean, String)]) = {\n    var rddNewData: org.apache.spark.rdd.RDD[((Int, Int,String), (Date, BigDecimal, BigDecimal))] = null\n    var rddFilteredMonthly: org.apache.spark.rdd.RDD[(String, String, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal,\n      BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, Boolean, String)] = rddMonthly\n\n    if (full || lastUpdateDate == null) {\n      rddNewData = rddStgFund.map(line => {\n        val (time, open, close, market, startTime) = line\n        val cal = Calendar.getInstance\n        cal.setTime(time)\n        ((cal.get(Calendar.YEAR), cal.get(Calendar.MONTH),market), (time, open, close))\n      })\n    }\n    else {\n      val years = rddStgFund.filter(line => {\n        val stgMarket = line._4\n        val loadDate = line._5\n        stgMarket == market && loadDate.after(lastUpdateDate)\n      }).map(line => {\n        val cal = Calendar.getInstance\n        cal.setTime(line._1)\n        cal.get(Calendar.YEAR)\n      }).distinct().collect()\n\n      val rddStgNewFund = rddStgFund.filter(line => {\n        val stgMarket = line._4\n        val cal = Calendar.getInstance\n        cal.setTime(line._1)\n        (years contains cal.get(Calendar.YEAR)) && stgMarket == market\n      })\n\n      rddNewData = rddStgNewFund.map(line => {\n        val (time, open, close, market, startTime) = line\n        val cal = Calendar.getInstance\n        cal.setTime(line._1)\n        ((cal.get(Calendar.YEAR), cal.get(Calendar.MONTH), market), (time, open, close))\n      })\n      rddFilteredMonthly = rddMonthly.filter(line => ! {\n        val monthlyMarket = line._1\n        val year = line._2\n        val monthlyOpenClose = line._16\n        (year == \"total\" || (years contains year.toInt)) && monthlyMarket == market\n      })\n    }\n\n    (rddNewData, rddFilteredMonthly)\n  }\n\n  def calculatePercents(rddNewData: org.apache.spark.rdd.RDD[((Int, Int,String), (Date, BigDecimal, BigDecimal))]):\n  org.apache.spark.rdd.RDD[((Int, String,Boolean), (Int, BigDecimal))] = {\n    val rddMinDateInMonth = rddNewData.reduceByKey((x, y) => {\n      if (x._1.before(y._1)) {\n        x\n      }\n      else {\n        y\n      }\n    })\n\n    val rddMaxDateInMonth = rddNewData.reduceByKey((x, y) => {\n      if (x._1.after(y._1)) {\n        x\n      }\n      else {\n        y\n      }\n    })\n\n    //((cal.get(Calendar.YEAR), cal.get(Calendar.MONTH),market), (time, open, close))\n    val rddOpenClose = rddMinDateInMonth.join(rddMaxDateInMonth).sortByKey()\n    var prevClose = BigDecimal(-1)\n    val rddPrevClose = rddOpenClose.map(line => {\n      val ((year, month, market), ((minTime, minOpen, minClose), (maxTime, maxOpen, maxClose))) = line\n      val temp = prevClose\n      prevClose = maxClose\n      if (temp != -1) {\n        (year, month, minOpen, maxClose, temp, market)\n      }\n      else {\n        (year, month, minOpen, maxClose, minOpen, market)\n      }\n    })\n\n    var rddPercents: org.apache.spark.rdd.RDD[((Int, String, Boolean), (Int, BigDecimal))] = null\n\n    val rddOpenClosePercents = rddOpenClose.map(line => {\n      val ((year, month, market), ((minTime, minOpen, minClose), (maxTime, maxOpen, maxClose))) = line\n      ((year, market, true), (month, (maxClose - minOpen) / minOpen))\n    }\n    )\n\n    val rddPrevClosePercents = rddPrevClose.map(line => {\n      val (year, month, open, close, prevClose, market) = line\n      ((year, market, false), (month, (close - prevClose) / prevClose))\n    })\n\n    rddPercents = rddOpenClosePercents.union(rddPrevClosePercents)\n    rddPercents\n  }\n\n  def createPivotTable(sc: SparkContext, rddPercents: org.apache.spark.rdd.RDD[((Int, String,Boolean), (Int, BigDecimal))],\n                       rddMonthly: org.apache.spark.rdd.RDD[(String, String, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal,\n                         BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, Boolean, String)],\n                       startTimestamp: String, argMarket: String):\n  org.apache.spark.rdd.RDD[(String, String, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal,\n    BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, Boolean, String)] = {\n\n    val rddPivotTable = rddPercents.reduceByKey((x, y) => {\n      if (x.isInstanceOf[ (Int, scala.math.BigDecimal)]) {\n        Array(x) ++ Array(y)\n      }\n      else {\n        x ++ Array(y)\n      }\n    }).map(line => {\n      //val ((year, market,typeOfPreviousValue), (month, percent) = line\n      var jan, feb, march, april, may, june, july, aug, sep, oct, nov, dec, total: BigDecimal = BigDecimal(0)\n      val (year, market, typeOfPreviousValue) = line._1\n      for (value <- line._2) {\n        val (month, percent) = value\n        month match {\n          case 0 => jan += percent\n          case 1 => feb += percent\n          case 2 => march += percent\n          case 3 => april += percent\n          case 4 => may += percent\n          case 5 => june += percent\n          case 6 => july += percent\n          case 7 => aug += percent\n          case 8 => sep += percent\n          case 9 => oct += percent\n          case 10 => nov += percent\n          case 11 => dec += percent\n        }\n        total += percent\n      }\n      (argMarket, year.toString, jan, feb, march, april, may, june, july, aug, sep, oct, nov, dec, total, typeOfPreviousValue, startTimestamp)\n    })\n\n    var rddResult = rddPivotTable.union(rddMonthly)\n\n    val count = BigDecimal(rddResult.filter(line => {\n      val monthlyMarket = line._1\n      monthlyMarket == argMarket\n    }).count() / 2L)\n\n    val rddOpenTotal = sc.parallelize(Array(rddResult.filter(line => {\n      val monthlyMarket = line._1\n      val monthlyOpenClose = line._16\n      monthlyMarket == argMarket && monthlyOpenClose == true\n    }).reduce((x, y) => (argMarket, \"total\", (x._3 + y._3) / count, (x._4 + y._4) / count, (x._5 + y._5) / count,\n      (x._6 + y._6) / count, (x._7 + y._7) / count, (x._8 + y._8) / count, (x._9 + y._9) / count, (x._10 + y._10) / count,\n      (x._11 + y._11) / count, (x._12 + y._12) / count, (x._13 + y._13) / count, (x._14 + y._14) / count,\n      (x._15 + y._15) / count, x._16, startTimestamp))))\n\n    rddResult = rddResult.union(rddOpenTotal)\n\n    val rddCloseTotal = sc.parallelize(Array(rddResult.filter(line => {\n      val monthlyMarket = line._1\n      val monthlyOpenClose = line._16\n      monthlyMarket == argMarket && monthlyOpenClose == true\n    }).reduce((x, y) => (argMarket, \"total\", (x._3 + y._3) / count, (x._4 + y._4) / count, (x._5 + y._5) / count,\n      (x._6 + y._6) / count, (x._7 + y._7) / count, (x._8 + y._8) / count, (x._9 + y._9) / count, (x._10 + y._10) / count,\n      (x._11 + y._11) / count, (x._12 + y._12) / count, (x._13 + y._13) / count, (x._14 + y._14) / count,\n      (x._15 + y._15) / count, x._16, startTimestamp))))\n\n    rddResult = rddResult.union(rddCloseTotal)\n\n    rddResult\n  }\n\n  def etlFromStgToDestination(sc: SparkContext, full: Boolean, market: String, openClose: Boolean): Unit = {\n    val startTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\n    val fs = FileSystem.get(new Configuration())\n    var rddMonthly = sc.emptyRDD[(String, String, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, Boolean, String)]\n    var rddStgFund: org.apache.spark.rdd.RDD[(java.util.Date, scala.math.BigDecimal, scala.math.BigDecimal, String, java.util.Date)] = null\n    var rddStgAudit: org.apache.spark.rdd.RDD[Array[String]] = null\n    var rddFundAudit: org.apache.spark.rdd.RDD[Array[String]] = null\n    var newAuditRecord: Array[String] = null\n\n    try {\n      newAuditRecord = Array(startTimestamp, null, \"monthly overview\", market)\n\n      rddStgFund = sc.textFile(\"/tmp/stg_fund/stg_fund_table/part*\").map(line => {\n        try {\n          val values = line.split(\";\")\n          val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n          val time = format.parse(values(0))\n          val open = BigDecimal(values(1))\n          val close = BigDecimal(values(2))\n          val market = values(3)\n          val startTimestamp = format.parse(values(4))\n          (time, open, close, market, startTimestamp)\n        }\n        catch {\n          case e => {\n            (null, BigDecimal(0), BigDecimal(0), null, null)\n          }\n        }\n      })\n\n      if (!full) {\n        if (fs.exists(new Path(\"/tmp/fund/monthly_table\"))) {\n          // market, year.toString, jan, feb, march, april, may, june, july, aug, sep, oct, nov, dec, total, typeOfPreviousValue, startTimestamp\n          rddMonthly = sc.textFile(\"/tmp/fund/monthly_table/part*\").map(line => {\n            val values = line.split(\";\")\n            (values(0), values(1), BigDecimal(values(2)), BigDecimal(values(3)), BigDecimal(values(4)), BigDecimal(values(5)),\n              BigDecimal(values(6)), BigDecimal(values(7)), BigDecimal(values(8)), BigDecimal(values(9)), BigDecimal(values(10)),\n              BigDecimal(values(11)), BigDecimal(values(12)), BigDecimal(values(13)), BigDecimal(values(14)), values(15).toBoolean, values(16))\n          })\n        }\n      }\n\n      rddStgAudit = sc.textFile(\"/tmp/audit/stg_audit_table/part*\").map(line => line.split(\";\"))\n      rddFundAudit = sc.textFile(\"/tmp/audit/fund_audit_table/part*\").map(line => line.split(\";\"))\n\n      val lastUpdateDate = findLastUpdateDate(rddFundAudit)\n\n      rddStgFund = getSuccessfulUploadedData(rddStgFund, rddStgAudit)\n\n\n      val (rddNewData, rddOldMonthly) = getDataToCalculate(rddStgFund, rddMonthly, lastUpdateDate, full, market)\n      \n      rddMonthly= rddOldMonthly\n\n      val rddPercents = calculatePercents(rddNewData)\n\n      val rddResult = createPivotTable(sc, rddPercents,  rddMonthly, startTimestamp: String, market: String)\n\n      rddResult.map(line => line.productIterator.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\n\n      fs.delete(new Path(\"/tmp/fund/monthly_table\"), true)\n      fs.rename(new Path(\"/tmp/tmp_file\"), new Path(\"/tmp/fund/monthly_table\"))\n      \n      val endTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\n      newAuditRecord = Array(startTimestamp, endTimestamp, \"monthly overview\", market)\n    }\n    catch {\n      case e => {\n        println(e)\n      }\n    }\n    finally {\n      addRecordInAuditTable(sc, rddFundAudit, newAuditRecord)\n    }\n  }\n}",
      "user": "anonymous",
      "dateUpdated": "2021-07-13T07:19:53+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "<console>:276: \u001b[33mwarning: \u001b[0mThis catches all Throwables. If this is really intended, use `case _ : Throwable` to clear this warning.\n                   case _ => {\n                        ^\n<console>:407: \u001b[31merror: \u001b[0mtype mismatch;\n found   : Array[(Int, scala.math.BigDecimal)]\n required: (Int, scala.math.BigDecimal)\n               Array(x) ++ Array(y)\n                        ^\n<console>:410: \u001b[31merror: \u001b[0mvalue ++ is not a member of (Int, scala.math.BigDecimal)\n               x ++ Array(y)\n                 ^\n<console>:493: \u001b[33mwarning: \u001b[0mThis catches all Throwables. If this is really intended, use `case e : Throwable` to clear this warning.\n                 case e => {\n                      ^\n<console>:536: \u001b[33mwarning: \u001b[0mThis catches all Throwables. If this is really intended, use `case e : Throwable` to clear this warning.\n             case e => {\n                  ^\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1626137315979_1026541607",
      "id": "paragraph_1626137315979_1026541607",
      "dateCreated": "2021-07-13T00:48:35+0000",
      "dateStarted": "2021-07-13T07:19:53+0000",
      "dateFinished": "2021-07-13T07:19:54+0000",
      "status": "ERROR",
      "$$hashKey": "object:115830"
    },
    {
      "text": "%spark\n\nsc.textFile(\"/tmp/stg_fund/stg_fund_table/part*\").collect()",
      "user": "anonymous",
      "dateUpdated": "2021-07-07T14:37:37+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres61\u001b[0m: \u001b[1m\u001b[32mArray[String]\u001b[0m = Array(2012-01-16 00:00:00;1291.4;1291.5;XAUUSD;2021-07-07 14:33:15, 2012-01-16 00:00:10;1291.4;1291.4;XAUUSD;2021-07-07 14:33:15, 2012-01-16 00:00:20;1291.6;1291.6;XAUUSD;2021-07-07 14:33:15, 2012-01-16 00:00:30;1291.5;1291.5;XAUUSD;2021-07-07 14:33:15, 2012-01-16 00:00:40;1291.1;1291.4;XAUUSD;2021-07-07 14:33:15, 2012-01-16 00:00:50;1291.4;1291.4;XAUUSD;2021-07-07 14:33:15, 2012-01-16 00:01:00;1291.1;1291.1;XAUUSD;2021-07-07 14:33:15, 2012-01-16 00:01:10;1290.9;1290.9;XAUUSD;2021-07-07 14:33:15, 2012-01-16 00:01:20;1291.1;1291.1;XAUUSD;2021-07-07 14:33:15, 2012-01-16 00:01:30;1290.9;1290.9;XAUUSD;2021-07-07 14:33:15, 2012-01-16 00:01:40;1290.9;1290.9;XAUUSD;2021-07-07 14:33:15, 2012-01-16 00:01:50;1290.6;1290.6;XAUUSD;2021-07-07 14:33:15...\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=89",
              "$$hashKey": "object:121254"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625668563946_836885713",
      "id": "paragraph_1625668563946_836885713",
      "dateCreated": "2021-07-07T14:36:03+0000",
      "dateStarted": "2021-07-07T14:37:37+0000",
      "dateFinished": "2021-07-07T14:37:38+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115831"
    },
    {
      "text": "%spark\n\nimport java.time.LocalDateTime\nimport java.time.format.DateTimeFormatter\nimport java.io.File\n\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{FileSystem, Path}\nimport java.nio.file.{Files, Paths}\nimport java.text.SimpleDateFormat\nimport java.util.Date\nimport java.util.Calendar\n\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkContext\nimport org.apache.spark.sql.functions._\n\nimport scala.math.BigDecimal\n\nobject DestinationETLManager {\n  def startAudit(sc: SparkContext, startTimestamp: String, market: String): Unit = {\n    val fs = FileSystem.get(new Configuration())\n    var rddFundAudit: org.apache.spark.rdd.RDD[Array[String]] = null\n    if (fs.exists(new Path(\"/tmp/audit/fund_audit_table\"))) {\n      rddFundAudit = sc.textFile(\"/tmp/landing_fund/landing_fund_table/part*\").map(line => line.split(\";\"))\n    }\n    else {\n      rddFundAudit = sc.emptyRDD[Array[String]]\n    }\n    val rddNewFundAuditRecord = sc.parallelize(Array(Array(startTimestamp, null, \"monthly overview\", market)))\n    rddFundAudit = rddFundAudit.union(rddNewFundAuditRecord)\n\n    rddFundAudit.map(line => line.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\n    fs.delete(new Path(\"/tmp/audit/fund_audit_table\"), true)\n    fs.rename(new Path(\"/tmp/tmp_file\"), new Path(\"/tmp/audit/fund_audit_table\"))\n  }\n\n  def endAudit(sc: SparkContext, startTimestamp: String, market: String): Unit = {\n    val fs = FileSystem.get(new Configuration())\n    var rddFundAudit = sc.textFile(\"/tmp/audit/fund_audit_table/part*\").map(line => line.split(\";\"))\n    val endTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\n    val rddNewEndRecord = sc.parallelize(Array(Array(startTimestamp, endTimestamp, \"monthly overview\", market)))\n    rddFundAudit = rddFundAudit.filter(line => line(0) != startTimestamp)\n    rddFundAudit = rddFundAudit.union(rddNewEndRecord)\n\n    rddFundAudit.map(line => line.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\n    fs.delete(new Path(\"/tmp/audit/fund_audit_table\"), true)\n    fs.rename(new Path(\"/tmp/tmp_file\"), new Path(\"/tmp/audit/fund_audit_table\"))\n  }\n\n  def etlFromStgToDestination(sc: SparkContext, full: Boolean, market: String, openClose: Boolean): Unit = {\n    val startTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\n    val fs = FileSystem.get(new Configuration())\n\n    startAudit(sc, startTimestamp, market)\n\n    var rddMonthly = sc.emptyRDD[(String, String, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, Boolean, String)]\n    var rddStgFund: org.apache.spark.rdd.RDD[(java.util.Date, scala.math.BigDecimal, scala.math.BigDecimal, String, java.util.Date)] = null\n    if (fs.exists(new Path(\"/tmp/stg_fund/stg_fund_table\"))) {\n      rddStgFund = sc.textFile(\"/tmp/stg_fund/stg_fund_table/part*\").map(line => {\n        try {\n          val values = line.split(\";\")\n          val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n          val time = format.parse(values(0))\n          val open = BigDecimal(values(1))\n          val close = BigDecimal(values(2))\n          val market = values(3)\n          val startTimestamp = format.parse(values(4))\n          (time, open, close, market, startTimestamp)\n        }\n        catch {\n          case e => {\n            (null, BigDecimal(0), BigDecimal(0), null, null)\n          }\n        }\n      })\n    }\n    else {\n      rddStgFund = sc.emptyRDD[(java.util.Date, scala.math.BigDecimal, scala.math.BigDecimal, String, java.util.Date)]\n    }\n\n    if (!full) {\n      if (fs.exists(new Path(\"/tmp/fund/monthly_table\"))) {\n        rddMonthly = sc.textFile(\"/tmp/fund/monthly_table/part*\").map(line => {\n          val values = line.split(\";\")\n          (values(0), values(1), BigDecimal(values(2)), BigDecimal(values(3)), BigDecimal(values(4)), BigDecimal(values(5)),\n            BigDecimal(values(6)), BigDecimal(values(7)), BigDecimal(values(8)), BigDecimal(values(9)), BigDecimal(values(10)),\n            BigDecimal(values(11)), BigDecimal(values(12)), BigDecimal(values(13)), BigDecimal(values(14)), values(15).toBoolean, values(16))\n        })\n      }\n\n      val dates = rddMonthly.filter(line => {\n        val monthlyMarket = line._1\n        val monthlyOpenClose = line._16\n        monthlyMarket == market && monthlyOpenClose == openClose\n      }).map(line => {\n        val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n        val time = format.parse(line._17)\n        time\n      }).collect()\n\n      var lastDate: Date = null\n      if (!dates.isEmpty) {\n        lastDate = dates.reduceLeft((x, y) => {\n          if (x.after(y)) {\n            x\n          }\n          else {\n            y\n          }\n        })\n      }\n\n      val years = rddStgFund.filter(line => {\n        val stgMarket = line._4\n        val startTimestamp = line._5\n        stgMarket == market && (lastDate == null || startTimestamp.after(lastDate))\n      }).map(line => {\n        val cal = Calendar.getInstance\n        cal.setTime(line._1)\n        cal.get(Calendar.YEAR)\n      }).distinct().collect()\n\n      rddMonthly = rddMonthly.filter(line => ! {\n        val monthlyMarket = line._1\n        val year = line._2\n        val monthlyOpenClose = line._16\n        (year == \"total\" || (years contains year.toInt)) && monthlyMarket == market && monthlyOpenClose == openClose\n      })\n\n      rddStgFund = rddStgFund.filter(line => {\n        val stgMarket = line._4\n        val cal = Calendar.getInstance\n        cal.setTime(line._1)\n        (years contains cal.get(Calendar.YEAR)) && stgMarket == market\n      })\n    }\n    else {\n      rddStgFund = rddStgFund.filter(line => {\n        val stgMarket = line._4\n        stgMarket == market\n      })\n    }\n\n    val rddNewData = rddStgFund.map(line => {\n      val cal = Calendar.getInstance\n      cal.setTime(line._1)\n      ((cal.get(Calendar.YEAR), cal.get(Calendar.MONTH)), (line._1, line._2, line._3, line._4))\n    })\n\n    val rddMinDateInMonth = rddNewData.reduceByKey((x, y) => {\n      if (x._1.before(y._1)) {\n        x\n      }\n      else {\n        y\n      }\n    })\n    val rddMaxDateInMonth = rddNewData.reduceByKey((x, y) => {\n      if (x._1.after(y._1)) {\n        x\n      }\n      else {\n        y\n      }\n    })\n\n    val rddOpenClose = rddMinDateInMonth.join(rddMaxDateInMonth).sortByKey()\n    var prevClose = BigDecimal(-1)\n    val rddPrevClose = rddOpenClose.map(line => {\n      val temp = prevClose\n      prevClose = line._2._2._3\n      if (temp != -1) {\n        (line._1, line._2, temp)\n      }\n      else {\n        (line._1, line._2, line._2._1._2)\n      }\n    })\n\n    var rddPercents: org.apache.spark.rdd.RDD[(Int, (Int, BigDecimal))] = null\n    if (openClose) {\n      rddPercents = rddOpenClose.map(line => (line._1._1, (line._1._2, (line._2._2._3 - line._2._1._2) / line._2._1._2)))\n    }\n    else {\n      rddPercents = rddPrevClose.map(line => (line._1._1, (line._1._2, (line._2._2._3 - line._3) / line._3)))\n    }\n\n    val rddPivotTable = rddPercents.groupByKey().mapValues(value => value.toList).map(line => {\n      var jan, feb, march, april, may, june, july, aug, sep, oct, nov, dec, total : BigDecimal = BigDecimal(0)\n      for (value <- line._2) {\n        value._1 match {\n          case 0 => jan += value._2\n          case 1 => feb += value._2\n          case 2 => march += value._2\n          case 3 => april += value._2\n          case 4 => may += value._2\n          case 5 => june += value._2\n          case 6 => july += value._2\n          case 7 => aug += value._2\n          case 8 => sep += value._2\n          case 9 => oct += value._2\n          case 10 => nov += value._2\n          case 11 => dec += value._2\n        }\n        total += value._2\n      }\n      (market, line._1.toString, jan, feb, march, april, may, june, july, aug, sep, oct, nov, dec, total, openClose, startTimestamp)\n    })\n\n    var rddResult = rddPivotTable.union(rddMonthly)\n\n    val count = BigDecimal(rddResult.filter(line => {\n      val monthlyMarket = line._1\n      val monthlyOpenClose = line._16\n      monthlyMarket == market && monthlyOpenClose == openClose\n    }).count())\n\n    val rddTotal = sc.parallelize(Array(rddResult.filter(line => {\n      val monthlyMarket = line._1\n      val monthlyOpenClose = line._16\n      monthlyMarket == market && monthlyOpenClose == openClose\n    }).reduce((x, y) => (market, \"total\", (x._3 + y._3) / count, (x._4 + y._4) / count, (x._5 + y._5) / count,\n      (x._6 + y._6) / count, (x._7 + y._7) / count, (x._8 + y._8) / count, (x._9 + y._9) / count, (x._10 + y._10) / count,\n      (x._11 + y._11) / count, (x._12 + y._12) / count, (x._13 + y._13) / count, (x._14 + y._14) / count,\n      (x._15 + y._15) / count, openClose, startTimestamp))))\n\n    rddResult = rddResult.union(rddTotal)\n    rddResult.map(line => line.productIterator.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\n\n    fs.delete(new Path(\"/tmp/fund/monthly_table\"), true)\n    fs.rename(new Path(\"/tmp/tmp_file\"), new Path(\"/tmp/fund/monthly_table\"))\n\n    endAudit(sc, startTimestamp, market)\n  }\n}",
      "user": "anonymous",
      "dateUpdated": "2021-07-07T15:32:48+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "<console>:278: \u001b[33mwarning: \u001b[0mThis catches all Throwables. If this is really intended, use `case e : Throwable` to clear this warning.\n                 case e => {\n                      ^\nimport java.time.LocalDateTime\nimport java.time.format.DateTimeFormatter\nimport java.io.File\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{FileSystem, Path}\nimport java.nio.file.{Files, Paths}\nimport java.text.SimpleDateFormat\nimport java.util.Date\nimport java.util.Calendar\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkContext\nimport org.apache.spark.sql.functions._\nimport scala.math.BigDecimal\ndefined object DestinationETLManager\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625668651138_94827344",
      "id": "paragraph_1625668651138_94827344",
      "dateCreated": "2021-07-07T14:37:31+0000",
      "dateStarted": "2021-07-07T15:32:48+0000",
      "dateFinished": "2021-07-07T15:32:49+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115832"
    },
    {
      "text": "%spark\n\nDestinationETLManager.etlFromStgToDestination(sc,true,\"XAUUSD\",true)",
      "user": "anonymous",
      "dateUpdated": "2021-07-07T15:32:53+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=92",
              "$$hashKey": "object:121354"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=93",
              "$$hashKey": "object:121355"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=94",
              "$$hashKey": "object:121356"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=95",
              "$$hashKey": "object:121357"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=96",
              "$$hashKey": "object:121358"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=97",
              "$$hashKey": "object:121359"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625669927764_1835080452",
      "id": "paragraph_1625669927764_1835080452",
      "dateCreated": "2021-07-07T14:58:47+0000",
      "dateStarted": "2021-07-07T15:32:53+0000",
      "dateFinished": "2021-07-07T15:32:55+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115833"
    },
    {
      "text": "%spark\n\nval full=true\nval market=\"XAUUSD\"\nval openClose=true\n\nval startTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\n    val fs = FileSystem.get(new Configuration())\n\nvar rddMonthly = sc.emptyRDD[(String, String, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, BigDecimal, Boolean, String)]\n    var rddStgFund: org.apache.spark.rdd.RDD[(java.util.Date, scala.math.BigDecimal, scala.math.BigDecimal, String, java.util.Date)] = null\n    if (fs.exists(new Path(\"/tmp/stg_fund/stg_fund_table\"))) {\n      rddStgFund = sc.textFile(\"/tmp/stg_fund/stg_fund_table/part*\").map(line => {\n        try {\n          val values = line.split(\";\")\n          val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n          val time = format.parse(values(0))\n          val open = BigDecimal(values(1))\n          val close = BigDecimal(values(2))\n          val market = values(3)\n          val startTimestamp = format.parse(values(4))\n          (time, open, close, market, startTimestamp)\n        }\n        catch {\n          case e => {\n            (null, BigDecimal(0), BigDecimal(0), null, null)\n          }\n        }\n      })\n    }\n    else {\n      rddStgFund = sc.emptyRDD[(java.util.Date, scala.math.BigDecimal, scala.math.BigDecimal, String, java.util.Date)]\n    }\n\n    if (!full) {\n      if (fs.exists(new Path(\"/tmp/fund/monthly_table\"))) {\n        rddMonthly = sc.textFile(\"/tmp/fund/monthly_table/part*\").map(line => {\n          val values = line.split(\";\")\n          (values(0), values(1), BigDecimal(values(2)), BigDecimal(values(3)), BigDecimal(values(4)), BigDecimal(values(5)),\n            BigDecimal(values(6)), BigDecimal(values(7)), BigDecimal(values(8)), BigDecimal(values(9)), BigDecimal(values(10)),\n            BigDecimal(values(11)), BigDecimal(values(12)), BigDecimal(values(13)), BigDecimal(values(14)), values(15).toBoolean, values(16))\n        })\n      }\n\n      val dates = rddMonthly.filter(line => {\n        val monthlyMarket = line._1\n        val monthlyOpenClose = line._16\n        monthlyMarket == market && monthlyOpenClose == openClose\n      }).map(line => {\n        val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n        val time = format.parse(line._17)\n        time\n      }).collect()\n\n      var lastDate: Date = null\n      if (!dates.isEmpty) {\n        lastDate = dates.reduceLeft((x, y) => {\n          if (x.after(y)) {\n            x\n          }\n          else {\n            y\n          }\n        })\n      }\n\n      val years = rddStgFund.filter(line => {\n        val stgMarket = line._4\n        val startTimestamp = line._5\n        stgMarket == market && (lastDate == null || startTimestamp.after(lastDate))\n      }).map(line => {\n        val cal = Calendar.getInstance\n        cal.setTime(line._1)\n        cal.get(Calendar.YEAR)\n      }).distinct().collect()\n\n      rddMonthly = rddMonthly.filter(line => ! {\n        val monthlyMarket = line._1\n        val year = line._2\n        val monthlyOpenClose = line._16\n        (year == \"total\" || (years contains year.toInt)) && monthlyMarket == market && monthlyOpenClose == openClose\n      })\n\n      rddStgFund = rddStgFund.filter(line => {\n        val stgMarket = line._4\n        val cal = Calendar.getInstance\n        cal.setTime(line._1)\n        (years contains cal.get(Calendar.YEAR)) && stgMarket == market\n      })\n    }\n    else {\n      rddStgFund = rddStgFund.filter(line => {\n        val stgMarket = line._4\n        stgMarket == market\n      })\n    }",
      "user": "anonymous",
      "dateUpdated": "2021-07-07T15:30:53+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "<console>:211: \u001b[33mwarning: \u001b[0mThis catches all Throwables. If this is really intended, use `case e : Throwable` to clear this warning.\n                 case e => {\n                      ^\n\u001b[1m\u001b[34mfull\u001b[0m: \u001b[1m\u001b[32mBoolean\u001b[0m = true\n\u001b[1m\u001b[34mmarket\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = XAUUSD\n\u001b[1m\u001b[34mopenClose\u001b[0m: \u001b[1m\u001b[32mBoolean\u001b[0m = true\n\u001b[1m\u001b[34mstartTimestamp\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = 2021-07-07 15:30:54\n\u001b[1m\u001b[34mfs\u001b[0m: \u001b[1m\u001b[32morg.apache.hadoop.fs.FileSystem\u001b[0m = DFS[DFSClient[clientName=DFSClient_NONMAPREDUCE_-406628818_18, ugi=zeppelin (auth:SIMPLE)]]\n\u001b[1m\u001b[34mrddMonthly\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(String, String, scala.math.BigDecimal, scala.math.BigDecimal, scala.math.BigDecimal, scala.math.BigDecimal, scala.math.BigDecimal, scala.math.BigDecimal, scala.math.BigDecimal, scala.math.BigDecimal, scala.math.BigDecimal, scala.math.BigDecimal, scala.math.BigDecimal, scala.math.BigDecimal, scala.math.BigDecimal, Boolean, String)]\u001b[0m = EmptyRDD[478] at empty...\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625669973447_1514120188",
      "id": "paragraph_1625669973447_1514120188",
      "dateCreated": "2021-07-07T14:59:33+0000",
      "dateStarted": "2021-07-07T15:30:53+0000",
      "dateFinished": "2021-07-07T15:30:54+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115834"
    },
    {
      "text": "%spark\n\nvar rddMonthly = sc.textFile(\"/tmp/fund/monthly_table/part*\").map(line => {\n          val values = line.split(\";\")\n          (values(0), values(1), BigDecimal(values(2)), BigDecimal(values(3)), BigDecimal(values(4)), BigDecimal(values(5)),\n            BigDecimal(values(6)), BigDecimal(values(7)), BigDecimal(values(8)), BigDecimal(values(9)), BigDecimal(values(10)),\n            BigDecimal(values(11)), BigDecimal(values(12)), BigDecimal(values(13)), BigDecimal(values(14)), values(15).toBoolean, values(16))\n        })",
      "user": "anonymous",
      "dateUpdated": "2021-07-07T15:03:10+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mrddMonthly\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.rdd.RDD[(String, String, scala.math.BigDecimal, scala.math.BigDecimal, scala.math.BigDecimal, scala.math.BigDecimal, scala.math.BigDecimal, scala.math.BigDecimal, scala.math.BigDecimal, scala.math.BigDecimal, scala.math.BigDecimal, scala.math.BigDecimal, scala.math.BigDecimal, scala.math.BigDecimal, scala.math.BigDecimal, Boolean, String)]\u001b[0m = MapPartitionsRDD[449] at map at <console>:163\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625670090168_2145515190",
      "id": "paragraph_1625670090168_2145515190",
      "dateCreated": "2021-07-07T15:01:30+0000",
      "dateStarted": "2021-07-07T15:03:10+0000",
      "dateFinished": "2021-07-07T15:03:10+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115835"
    },
    {
      "text": "%spark\r\n\r\nimport java.time.LocalDateTime\r\nimport java.time.format.DateTimeFormatter\r\nimport java.io.File\r\n\r\nimport org.apache.hadoop.conf.Configuration\r\nimport org.apache.hadoop.fs.{FileSystem, Path}\r\nimport java.security.{DigestInputStream, MessageDigest}\r\nimport java.nio.file.{Files, Paths}\r\nimport java.text.SimpleDateFormat\r\n\r\nimport org.apache.spark.SparkContext._\r\nimport org.apache.spark.SparkContext\r\nimport org.apache.spark.sql.functions._\r\n\r\nimport scala.math.BigDecimal\r\n\r\nobject StgETLManager {\r\n\r\n  def checkSumSha256(path: Path): String = {\r\n    val hdfs = FileSystem.get(new Configuration())\r\n\r\n    val buffer = new Array[Byte](8192)\r\n    val sha256 = MessageDigest.getInstance(\"SHA-256\")\r\n\r\n    val dis = new DigestInputStream(hdfs.open(path), sha256)\r\n    try {\r\n      while (dis.read(buffer) != -1) {}\r\n    }\r\n    finally {\r\n      dis.close()\r\n    }\r\n\r\n    sha256.digest.map(\"%02x\".format(_)).mkString\r\n  }\r\n\r\n  def addRecordInAuditTable(sc: SparkContext, rddStgAudit: org.apache.spark.rdd.RDD[Array[String]], newRecord: Array[String]): Unit = {\r\n    val fs = FileSystem.get(new Configuration())\r\n\r\n    val rddNewStartRecord = sc.parallelize(Array(newRecord))\r\n    rddStgAudit.union(rddNewStartRecord).map(line => line.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_audit_file\")\r\n\r\n    fs.delete(new Path(\"/tmp/audit/stg_audit_table\"), true)\r\n    fs.rename(new Path(\"/tmp/tmp_audit_file\"), new Path(\"/tmp/audit/stg_audit_table\"))\r\n    fs.delete(new Path(\"/tmp/tmp_audit_file\"), true)\r\n  }\r\n\r\n\r\n  def etlFromSourceToStg(sc: SparkContext, market: String, full: Boolean): Unit = {\r\n    var startTimestamp: String = null\r\n    val fs = FileSystem.get(new Configuration())\r\n    val files = fs.listStatus(new Path(\"/tmp/data/XAUUSD\"))\r\n    var rddStgFund: org.apache.spark.rdd.RDD[(java.util.Date, scala.math.BigDecimal, scala.math.BigDecimal, java.util.Date, String)] = null\r\n    var count: Long = 0L\r\n    var rddStgAudit: org.apache.spark.rdd.RDD[Array[String]] = null\r\n\r\n    if (fs.exists(new Path(\"/tmp/audit/stg_audit_table\"))) {\r\n      rddStgAudit = sc.textFile(\"/tmp/audit/stg_audit_table/part*\").map(line => line.split(\";\"))\r\n    }\r\n    else {\r\n      rddStgAudit = sc.emptyRDD[Array[String]]\r\n    }\r\n\r\n    if (full && fs.exists(new Path(\"/tmp/stg_fund/stg_fund_table\"))) {\r\n      fs.delete(new Path(\"/tmp/stg_fund/stg_fund_table\"), true)\r\n    }\r\n\r\n    var newAuditRecord: Array[String] = null\r\n\r\n    for (file <- files) {\r\n      if(fs.isFile(file.getPath)) {\r\n        try {\r\n          startTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\r\n          val checkSum = checkSumSha256(file.getPath)\r\n          val checkLog = rddStgAudit.filter(line => line(1) == checkSum).take(1)\r\n\r\n          if (full || checkLog.isEmpty || (!checkLog.isEmpty && checkLog(0)(6) == \"null\")) {\r\n            newAuditRecord = Array(file.getPath.toString, checkSum, startTimestamp, market, \"load data from files\", \"stg_fund_db\", null, null)\r\n            //startAudit(sc,rddStgAudit file.getPath.toString, checkSum, startTimestamp, market)\r\n\r\n            rddStgFund = sc.textFile(file.getPath.toString).map(line => {\r\n              try {\r\n                val Array(rawTime, rawOpen, rawHigh, rawLow, rawClose, rawVolume) = line.split(\";\")\r\n                val format = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\r\n                val time = format.parse(rawTime)\r\n                val open = BigDecimal(rawOpen.replace(\",\", \".\"))\r\n                val close = BigDecimal(rawClose.replace(\",\", \".\"))\r\n                val parsedStartTimestamp = format.parse(startTimestamp)\r\n                (time, open, close, parsedStartTimestamp, market)\r\n              }\r\n              catch {\r\n                case _ => {\r\n                  (null, BigDecimal(0), BigDecimal(0), null, null)\r\n                }\r\n              }\r\n            }).filter(line => line._1 != null && line._2 != 0 && line._3 != 0 && line._4 != null && line._5 != null)\r\n            count = rddStgFund.count()\r\n\r\n            if (fs.exists(new Path(\"/tmp/stg_fund/stg_fund_table\"))) {\r\n              rddStgFund.map(line => line.productIterator.mkString(\";\")).saveAsTextFile(\"/tmp/tmp_file\")\r\n              val tmpFiles = fs.listStatus(new Path(\"/tmp/tmp_file\"))\r\n              for (file <- tmpFiles) {\r\n                  val packages = file.getPath.toString.split(\"/\")\r\n                 val filename = packages(packages.size-1)\r\n//                fs.copyFromLocalFile(file.getPath, new Path(s\"/tmp/stg_fund/stg_fund_table/'$(filename+checkSum)'\"))\r\n                fs.rename(file.getPath,new Path(\"/tmp/stg_fund/stg_fund_table/\"+filename+checkSum))\r\n              }\r\n              //fs.copyFromLocalFile(new Path(\"/tmp/tmp_file/part*\"), new Path(\"/tmp/stg_fund/stg_fund_table/part*\"))\r\n              fs.delete(new Path(\"/tmp/tmp_file\"), true)\r\n            }\r\n            else {\r\n              rddStgFund.map(line => line.productIterator.mkString(\";\")).saveAsTextFile(\"/tmp/stg_fund/stg_fund_table\")\r\n            }\r\n\r\n            val endTimestamp = LocalDateTime.now.format(DateTimeFormatter.ofPattern(\"yyyy-MM-dd HH:mm:ss\"))\r\n            //endAudit(sc, rddStg_audit,file.getPath.toString, checkSum, startTimestamp, market, count.toString)\r\n            newAuditRecord = Array(file.getPath.toString, checkSum, startTimestamp, market, \"load data from files\", \"stg_fund_db\", endTimestamp, count.toString)\r\n          }\r\n        }\r\n        catch {\r\n          case e => {\r\n            println(e)\r\n          }\r\n        }\r\n        finally {\r\n          addRecordInAuditTable(sc, rddStgAudit, newAuditRecord)\r\n        }\r\n      }\r\n    }\r\n  }\r\n}\r\n",
      "user": "anonymous",
      "dateUpdated": "2021-07-12T21:56:43+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "<console>:286: \u001b[33mwarning: \u001b[0mThis catches all Throwables. If this is really intended, use `case _ : Throwable` to clear this warning.\n                       case _ => {\n                            ^\n<console>:315: \u001b[33mwarning: \u001b[0mThis catches all Throwables. If this is really intended, use `case e : Throwable` to clear this warning.\n                 case e => {\n                      ^\nimport java.time.LocalDateTime\nimport java.time.format.DateTimeFormatter\nimport java.io.File\nimport org.apache.hadoop.conf.Configuration\nimport org.apache.hadoop.fs.{FileSystem, Path}\nimport java.security.{DigestInputStream, MessageDigest}\nimport java.nio.file.{Files, Paths}\nimport java.text.SimpleDateFormat\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkContext\nimport org.apache.spark.sql.functions._\nimport scala.math.BigDecimal\ndefined object StgETLManager\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625670182669_326538217",
      "id": "paragraph_1625670182669_326538217",
      "dateCreated": "2021-07-07T15:03:02+0000",
      "dateStarted": "2021-07-12T21:56:43+0000",
      "dateFinished": "2021-07-12T21:56:44+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115836"
    },
    {
      "text": "%spark\n\nStgETLManager.etlFromSourceToStg(sc,\"XAUUSD\",true)",
      "user": "anonymous",
      "dateUpdated": "2021-07-12T21:56:48+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": []
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1070",
              "$$hashKey": "object:121583"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1071",
              "$$hashKey": "object:121584"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1072",
              "$$hashKey": "object:121585"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1073",
              "$$hashKey": "object:121586"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1074",
              "$$hashKey": "object:121587"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1075",
              "$$hashKey": "object:121588"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1076",
              "$$hashKey": "object:121589"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1077",
              "$$hashKey": "object:121590"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1078",
              "$$hashKey": "object:121591"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1079",
              "$$hashKey": "object:121592"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1080",
              "$$hashKey": "object:121593"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1081",
              "$$hashKey": "object:121594"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1082",
              "$$hashKey": "object:121595"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1083",
              "$$hashKey": "object:121596"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1084",
              "$$hashKey": "object:121597"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1085",
              "$$hashKey": "object:121598"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1086",
              "$$hashKey": "object:121599"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1087",
              "$$hashKey": "object:121600"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1088",
              "$$hashKey": "object:121601"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1089",
              "$$hashKey": "object:121602"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1090",
              "$$hashKey": "object:121603"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1091",
              "$$hashKey": "object:121604"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1092",
              "$$hashKey": "object:121605"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1093",
              "$$hashKey": "object:121606"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1094",
              "$$hashKey": "object:121607"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1095",
              "$$hashKey": "object:121608"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1096",
              "$$hashKey": "object:121609"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1097",
              "$$hashKey": "object:121610"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1098",
              "$$hashKey": "object:121611"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1099",
              "$$hashKey": "object:121612"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1100",
              "$$hashKey": "object:121613"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1101",
              "$$hashKey": "object:121614"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1102",
              "$$hashKey": "object:121615"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1103",
              "$$hashKey": "object:121616"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1104",
              "$$hashKey": "object:121617"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1105",
              "$$hashKey": "object:121618"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1106",
              "$$hashKey": "object:121619"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1107",
              "$$hashKey": "object:121620"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1108",
              "$$hashKey": "object:121621"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1109",
              "$$hashKey": "object:121622"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1110",
              "$$hashKey": "object:121623"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1111",
              "$$hashKey": "object:121624"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1112",
              "$$hashKey": "object:121625"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1113",
              "$$hashKey": "object:121626"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1114",
              "$$hashKey": "object:121627"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1115",
              "$$hashKey": "object:121628"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1116",
              "$$hashKey": "object:121629"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1117",
              "$$hashKey": "object:121630"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1118",
              "$$hashKey": "object:121631"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1119",
              "$$hashKey": "object:121632"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1120",
              "$$hashKey": "object:121633"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1121",
              "$$hashKey": "object:121634"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1122",
              "$$hashKey": "object:121635"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1123",
              "$$hashKey": "object:121636"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1124",
              "$$hashKey": "object:121637"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1125",
              "$$hashKey": "object:121638"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1126",
              "$$hashKey": "object:121639"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1127",
              "$$hashKey": "object:121640"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1128",
              "$$hashKey": "object:121641"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1129",
              "$$hashKey": "object:121642"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1130",
              "$$hashKey": "object:121643"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1131",
              "$$hashKey": "object:121644"
            },
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=1132",
              "$$hashKey": "object:121645"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1626123095699_2102442069",
      "id": "paragraph_1626123095699_2102442069",
      "dateCreated": "2021-07-12T20:51:35+0000",
      "dateStarted": "2021-07-12T21:56:48+0000",
      "dateFinished": "2021-07-12T22:01:32+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115837"
    },
    {
      "text": "%spark\n\nsc.textFile(\"/tmp/audit/stg_audit_table\").collect()",
      "user": "anonymous",
      "dateUpdated": "2021-07-12T21:11:46+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres304\u001b[0m: \u001b[1m\u001b[32mArray[String]\u001b[0m = Array(2021-07-04 22:53:47;2021-07-04 22:53:48;load data from landing to staging;1446, 2021-07-05 14:45:24;null;load data from landing to staging;0, 2021-07-05 18:36:40;null;load data from landing to staging;0, 2021-07-05 18:37:20;null;load data from landing to staging;0, 2021-07-05 18:35:49;null;load data from landing to staging;0, 2021-07-05 18:40:58;null;load data from landing to staging;0, 2021-07-07 14:36:04;2021-07-07 14:36:06;load data from landing to staging;1446, hdfs://cluster-cd71-m/tmp/data/XAUUSD/XAUUSD_10 Secs_Bid_2012.01.01_2012.12.31.csv;3bc31cf68bdec08994cae78c9e402e5b0190a94021448fbfa248a8538e14d1fa;2021-07-12 20:57:33;XAUUSD;load data from files;stg_fund_db;2021-07-12 20:58:07;2218315)\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:33221/jobs/job?id=847",
              "$$hashKey": "object:121951"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1626123746131_1926246233",
      "id": "paragraph_1626123746131_1926246233",
      "dateCreated": "2021-07-12T21:02:26+0000",
      "dateStarted": "2021-07-12T21:11:46+0000",
      "dateFinished": "2021-07-12T21:11:47+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115838"
    },
    {
      "text": "%spark\n\nsc.textFile(\"/tmp/fund/monthly_table/part*\").collect()",
      "user": "anonymous",
      "dateUpdated": "2021-07-07T15:32:59+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "\u001b[1m\u001b[34mres80\u001b[0m: \u001b[1m\u001b[32mArray[String]\u001b[0m = Array(XAUUSD;2012;0.01757782251819730525011615301223478;0;0;0;0;0;0;-0.00007802801915233197375421173967015428;0;-0.0003824304498077417829602898127481452;0;-0.0003294407917794008383216744003476652;0.01678792325745783065507997705946881532;true;2021-07-07 15:32:53, XAUUSD;2013;0;0;0;0;0;0;0;-0.0001772921810602308816976553700028721;0;0;0;0.02267161812401585912248106963872140;0.0224943259429556282407834142687185279;true;2021-07-07 15:32:53, XAUUSD;2014;0;0;0;0;0.0001474634975736262316198213867925856;0;0;0;-0.000008048556944043413916156170174663746;0;0;-0.001092130448914731481815883563870129;-0.000952715508285148664112218347252207146;true;2021-07-07 15:32:53, XAUUSD;2015;0;0;0;0;0;0;0;0;0;0;0;0;0;true;2021-07-07 15:32:53, XAUUSD;2016;0;0;0;0;0....\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {
        "jobUrl": {
          "propertyName": "jobUrl",
          "label": "SPARK JOB",
          "tooltip": "View in Spark web UI",
          "group": "spark",
          "values": [
            {
              "jobUrl": "http://cluster-cd71-m.europe-west6-a.c.perfect-trilogy-317510.internal:37163/jobs/job?id=98",
              "$$hashKey": "object:122009"
            }
          ],
          "interpreterSettingId": "spark"
        }
      },
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625670356515_347145836",
      "id": "paragraph_1625670356515_347145836",
      "dateCreated": "2021-07-07T15:05:56+0000",
      "dateStarted": "2021-07-07T15:32:59+0000",
      "dateFinished": "2021-07-07T15:32:59+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115839"
    },
    {
      "text": "%spark\n",
      "user": "anonymous",
      "dateUpdated": "2021-07-12T20:53:06+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1626123186823_2055170829",
      "id": "paragraph_1626123186823_2055170829",
      "dateCreated": "2021-07-12T20:53:06+0000",
      "status": "READY",
      "$$hashKey": "object:115840"
    },
    {
      "text": "%spark\nimport org.apache.hadoop.io.NullWritable\n\nimport org.apache.spark._\nimport org.apache.spark.SparkContext._\n\nimport org.apache.hadoop.mapred.lib.MultipleTextOutputFormat\n\nclass RDDMultipleTextOutputFormat extends MultipleTextOutputFormat[Any, Any] {\n  override def generateActualKey(key: Any, value: Any): Any = \n    NullWritable.get()\n\n  override def generateFileNameForKeyValue(key: Any, value: Any, name: String): String = \n    key.asInstanceOf[String]\n}\n\nobject Split {\n  def main(args: Array[String]) {\n    sc.makeRDD(Seq((1, \"a\"), (1, \"b\"), (2, \"c\")))\n    .partitionBy(new HashPartitioner(1))\n    .saveAsHadoopFile(\"output/path\", classOf[String], classOf[String],\n      classOf[RDDMultipleTextOutputFormat])\n    spark.stop()\n  }\n}",
      "user": "anonymous",
      "dateUpdated": "2021-07-12T10:36:48+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "import org.apache.hadoop.io.NullWritable\nimport org.apache.spark._\nimport org.apache.spark.SparkContext._\nimport org.apache.hadoop.mapred.lib.MultipleTextOutputFormat\ndefined class RDDMultipleTextOutputFormat\ndefined object Split\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1625671921425_1676255087",
      "id": "paragraph_1625671921425_1676255087",
      "dateCreated": "2021-07-07T15:32:01+0000",
      "dateStarted": "2021-07-12T10:36:48+0000",
      "dateFinished": "2021-07-12T10:36:49+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115841"
    },
    {
      "text": "%spark\n\nimport RDDMultipleTextOutputFormat._\n\nval testRDD = sc.makeRDD(Seq((1, \"a\"), (1, \"b\"), (2, \"c\")))\ntestRDD.partitionBy(new HashPartitioner(1))\n    .saveAsHadoopFile(\"/tmp/test_partitions/path\", classOf[String], classOf[String],\n      classOf[RDDMultipleTextOutputFormat])",
      "user": "anonymous",
      "dateUpdated": "2021-07-12T10:34:57+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "ERROR",
        "msg": [
          {
            "type": "TEXT",
            "data": "<console>:66: \u001b[31merror: \u001b[0mnot found: value RDDMultipleTextOutputFormat\n       import RDDMultipleTextOutputFormat._\n              ^\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1626085785336_1313329747",
      "id": "paragraph_1626085785336_1313329747",
      "dateCreated": "2021-07-12T10:29:45+0000",
      "dateStarted": "2021-07-12T10:34:57+0000",
      "dateFinished": "2021-07-12T10:34:57+0000",
      "status": "ERROR",
      "$$hashKey": "object:115842"
    },
    {
      "text": "%spark\n\nprintln((1,2,3))",
      "user": "anonymous",
      "dateUpdated": "2021-07-12T10:58:59+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "results": {
        "code": "SUCCESS",
        "msg": [
          {
            "type": "TEXT",
            "data": "(1,2,3)\n"
          }
        ]
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1626086059621_819876368",
      "id": "paragraph_1626086059621_819876368",
      "dateCreated": "2021-07-12T10:34:19+0000",
      "dateStarted": "2021-07-12T10:58:59+0000",
      "dateFinished": "2021-07-12T10:59:00+0000",
      "status": "FINISHED",
      "$$hashKey": "object:115843"
    },
    {
      "text": "%spark\n",
      "user": "anonymous",
      "dateUpdated": "2021-07-12T10:58:59+0000",
      "config": {
        "colWidth": 12,
        "fontSize": 9,
        "enabled": true,
        "results": {},
        "editorSetting": {
          "language": "scala",
          "editOnDblClick": false,
          "completionKey": "TAB",
          "completionSupport": true
        },
        "editorMode": "ace/mode/scala"
      },
      "settings": {
        "params": {},
        "forms": {}
      },
      "apps": [],
      "runtimeInfos": {},
      "progressUpdateIntervalMs": 500,
      "jobName": "paragraph_1626087539859_1176608546",
      "id": "paragraph_1626087539859_1176608546",
      "dateCreated": "2021-07-12T10:58:59+0000",
      "status": "READY",
      "$$hashKey": "object:115844"
    }
  ],
  "name": "test",
  "id": "2G95VUHBF",
  "defaultInterpreterGroup": "spark",
  "version": "0.9.0-preview2",
  "noteParams": {},
  "noteForms": {},
  "angularObjects": {},
  "config": {
    "isZeppelinNotebookCronEnable": false,
    "looknfeel": "default",
    "personalizedMode": "false"
  },
  "info": {},
  "path": "/test"
}